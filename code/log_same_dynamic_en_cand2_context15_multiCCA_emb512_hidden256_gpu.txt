WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:
 https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29

Using gpu device 1: GeForce GTX 1070 (CNMeM is disabled, cuDNN 5105)

Namespace(activation='tanh', attention=0, batch=32, data_size=10000000, dev_data='../data/input/dev-data.cand-2.gz', dim_emb=512, dim_hidden=256, emb_type='multi', epoch=30, init_emb='/cl/work/motoki-s/multi_ling_conversation/ouchi/dialog-emnlp2016/DATA-multi/embeddings/fifty_nine.table5.multiCCA.size_512+w_5+it_10.normalized', lang='en', load_param=None, loss='nll', lr=0.001, max_n_words=20, mode='train', model='dynamic', n_cands=2, n_prev_sents=15, opt='adam', output=0, output_fn='same_dynamic_lang_en_cand2_context15_multiCCA_emb512_hidden256', reg=0.0001, sample_size=1, save=1, test_data='../data/input/test-data.cand-2.gz', train_data='../data/input/train-data.cand-2.gz', unit='gru')


ADDRESSEE AND RESPONSE SELECTION SYSTEM START

SET UP DATASET

Load dataset...
Load initial word embedding...
	Word Embedding Size: 176692

TASK  SETTING
	Response Candidates:2  Contexts:15  Max Word Num:20


Converting words into ids...
	Questions:   737343
	Questions:    46313
	Questions:    53037

Creating samples...
	THREADS:  6606
	  SAMPLES:   665689
	  ADDRESSEE DETECTION CHANCE LEVEL:   1.44%
	  ADDRESSEE DETECTION UPPER BOUND:
		# Cands  1:   0.00% | Total:        0 | Including true-adr:        0 | Not including:        0
		# Cands  2: 100.00% | Total:    43321 | Including true-adr:    43321 | Not including:        0
		# Cands  3: 100.00% | Total:    94225 | Including true-adr:    94225 | Not including:        0
		# Cands  4: 100.00% | Total:   126407 | Including true-adr:   126407 | Not including:        0
		# Cands  5: 100.00% | Total:   129253 | Including true-adr:   129253 | Not including:        0
		# Cands  6: 100.00% | Total:   111528 | Including true-adr:   111528 | Not including:        0
		# Cands  7: 100.00% | Total:    80066 | Including true-adr:    80066 | Not including:        0
		# Cands  8: 100.00% | Total:    47046 | Including true-adr:    47046 | Not including:        0
		# Cands  9: 100.00% | Total:    22226 | Including true-adr:    22226 | Not including:        0
		# Cands 10: 100.00% | Total:     8195 | Including true-adr:     8195 | Not including:        0
		# Cands 11: 100.00% | Total:     2607 | Including true-adr:     2607 | Not including:        0
		# Cands 12: 100.00% | Total:      642 | Including true-adr:      642 | Not including:        0
		# Cands 13: 100.00% | Total:      151 | Including true-adr:      151 | Not including:        0
		# Cands 14: 100.00% | Total:       18 | Including true-adr:       18 | Not including:        0
		# Cands 15: 100.00% | Total:        2 | Including true-adr:        2 | Not including:        0
		# Cands 16: 100.00% | Total:        2 | Including true-adr:        2 | Not including:        0

	  THE BINNED NUMBER OF AGENTS IN CONTEXT:
		Bin  0:    37558
		Bin  1:    92817
		Bin  2:    91833
		Bin  3:    74396
		Bin  4:    78071
		Bin  5:    96609
		Bin  6:   194405

	THREADS:   367
	  SAMPLES:    45132
	  ADDRESSEE DETECTION CHANCE LEVEL:   1.24%
	  ADDRESSEE DETECTION UPPER BOUND:
		# Cands  1:   0.00% | Total:        2 | Including true-adr:        0 | Not including:        2
		# Cands  2:  97.68% | Total:     2759 | Including true-adr:     2695 | Not including:       64
		# Cands  3:  96.47% | Total:     5913 | Including true-adr:     5704 | Not including:      209
		# Cands  4:  95.83% | Total:     7820 | Including true-adr:     7494 | Not including:      326
		# Cands  5:  96.27% | Total:     8583 | Including true-adr:     8263 | Not including:      320
		# Cands  6:  96.65% | Total:     7930 | Including true-adr:     7664 | Not including:      266
		# Cands  7:  96.77% | Total:     5791 | Including true-adr:     5604 | Not including:      187
		# Cands  8:  96.84% | Total:     3768 | Including true-adr:     3649 | Not including:      119
		# Cands  9:  96.66% | Total:     1739 | Including true-adr:     1681 | Not including:       58
		# Cands 10:  96.86% | Total:      606 | Including true-adr:      587 | Not including:       19
		# Cands 11:  98.27% | Total:      173 | Including true-adr:      170 | Not including:        3
		# Cands 12:  94.59% | Total:       37 | Including true-adr:       35 | Not including:        2
		# Cands 13: 100.00% | Total:       11 | Including true-adr:       11 | Not including:        0
		# Cands 14:   0.00% | Total:        0 | Including true-adr:        0 | Not including:        0
		# Cands 15:   0.00% | Total:        0 | Including true-adr:        0 | Not including:        0
		# Cands 16:   0.00% | Total:        0 | Including true-adr:        0 | Not including:        0

	  THE BINNED NUMBER OF AGENTS IN CONTEXT:
		Bin  0:     3400
		Bin  1:     5442
		Bin  2:     5286
		Bin  3:     4253
		Bin  4:     4507
		Bin  5:     6883
		Bin  6:    15361

	THREADS:   382
	  SAMPLES:    51897
	  ADDRESSEE DETECTION CHANCE LEVEL:   1.24%
	  ADDRESSEE DETECTION UPPER BOUND:
		# Cands  1:   0.00% | Total:        0 | Including true-adr:        0 | Not including:        0
		# Cands  2:  97.12% | Total:     3263 | Including true-adr:     3169 | Not including:       94
		# Cands  3:  96.16% | Total:     6764 | Including true-adr:     6504 | Not including:      260
		# Cands  4:  96.08% | Total:     9403 | Including true-adr:     9034 | Not including:      369
		# Cands  5:  96.65% | Total:    10106 | Including true-adr:     9767 | Not including:      339
		# Cands  6:  96.66% | Total:     9141 | Including true-adr:     8836 | Not including:      305
		# Cands  7:  96.83% | Total:     6562 | Including true-adr:     6354 | Not including:      208
		# Cands  8:  96.28% | Total:     3848 | Including true-adr:     3705 | Not including:      143
		# Cands  9:  96.42% | Total:     1872 | Including true-adr:     1805 | Not including:       67
		# Cands 10:  95.82% | Total:      670 | Including true-adr:      642 | Not including:       28
		# Cands 11:  93.30% | Total:      209 | Including true-adr:      195 | Not including:       14
		# Cands 12:  96.36% | Total:       55 | Including true-adr:       53 | Not including:        2
		# Cands 13: 100.00% | Total:        4 | Including true-adr:        4 | Not including:        0
		# Cands 14:   0.00% | Total:        0 | Including true-adr:        0 | Not including:        0
		# Cands 15:   0.00% | Total:        0 | Including true-adr:        0 | Not including:        0
		# Cands 16:   0.00% | Total:        0 | Including true-adr:        0 | Not including:        0

	  THE BINNED NUMBER OF AGENTS IN CONTEXT:
		Bin  0:     3731
		Bin  1:     5962
		Bin  2:     5475
		Bin  3:     4495
		Bin  4:     5619
		Bin  5:     7956
		Bin  6:    18659


TRAIN SETTING	Batch Size:32  Epoch:30  Vocab:176693  Max Words:20

Train samples	Mini-Batch:20798
Dev samples	Mini-Batch:1457
Test samples	Mini-Batch:1669

BUILD A MODEL
MODEL: dynamic  Unit: gru  Opt: adam  Activation: tanh  Parameters: 1245184


TRAINING START



Epoch: 1
  TRAIN    100/20798  200/20798  300/20798  400/20798  500/20798  600/20798  700/20798  800/20798  900/20798  1000/20798  1100/20798  1200/20798  1300/20798  1400/20798  1500/20798  1600/20798  1700/20798  1800/20798  1900/20798  2000/20798  2100/20798  2200/20798  2300/20798  2400/20798  2500/20798  2600/20798  2700/20798  2800/20798  2900/20798  3000/20798  3100/20798  3200/20798  3300/20798  3400/20798  3500/20798  3600/20798  3700/20798  3800/20798  3900/20798  4000/20798  4100/20798  4200/20798  4300/20798  4400/20798  4500/20798  4600/20798  4700/20798  4800/20798  4900/20798  5000/20798  5100/20798  5200/20798  5300/20798  5400/20798  5500/20798  5600/20798  5700/20798  5800/20798  5900/20798  6000/20798  6100/20798  6200/20798  6300/20798  6400/20798  6500/20798  6600/20798  6700/20798  6800/20798  6900/20798  7000/20798  7100/20798  7200/20798  7300/20798  7400/20798  7500/20798  7600/20798  7700/20798  7800/20798  7900/20798  8000/20798  8100/20798  8200/20798  8300/20798  8400/20798  8500/20798  8600/20798  8700/20798  8800/20798  8900/20798  9000/20798  9100/20798  9200/20798  9300/20798  9400/20798  9500/20798  9600/20798  9700/20798  9800/20798  9900/20798  10000/20798  10100/20798  10200/20798  10300/20798  10400/20798  10500/20798  10600/20798  10700/20798  10800/20798  10900/20798  11000/20798  11100/20798  11200/20798  11300/20798  11400/20798  11500/20798  11600/20798  11700/20798  11800/20798  11900/20798  12000/20798  12100/20798  12200/20798  12300/20798  12400/20798  12500/20798  12600/20798  12700/20798  12800/20798  12900/20798  13000/20798  13100/20798  13200/20798  13300/20798  13400/20798  13500/20798  13600/20798  13700/20798  13800/20798  13900/20798  14000/20798  14100/20798  14200/20798  14300/20798  14400/20798  14500/20798  14600/20798  14700/20798  14800/20798  14900/20798  15000/20798  15100/20798  15200/20798  15300/20798  15400/20798  15500/20798  15600/20798  15700/20798  15800/20798  15900/20798  16000/20798  16100/20798  16200/20798  16300/20798  16400/20798  16500/20798  16600/20798  16700/20798  16800/20798  16900/20798  17000/20798  17100/20798  17200/20798  17300/20798  17400/20798  17500/20798  17600/20798  17700/20798  17800/20798  17900/20798  18000/20798  18100/20798  18200/20798  18300/20798  18400/20798  18500/20798  18600/20798  18700/20798  18800/20798  18900/20798  19000/20798  19100/20798  19200/20798  19300/20798  19400/20798  19500/20798  19600/20798  19700/20798  19800/20798  19900/20798  20000/20798  20100/20798  20200/20798  20300/20798  20400/20798  20500/20798  20600/20798  20700/20798
	Time: 1449.591685
	Total Loss: 751641.458567	Total Grad Norm: 145558.686844
	Avg.  Loss: 36.140084	Avg.  Grad Norm: 6.998687

	Accuracy
	TOTAL  Both:  49.55% ( 329753/ 665536)  Adr:  68.24% ( 454147/ 665536)  Res:  71.71% ( 477285/ 665536)

	    0  Both:  60.20% (  22607/  37556)  Adr:  86.01% (  32301/  37556)  Res:  69.49% (  26099/  37556)
	    1  Both:  54.13% (  50237/  92804)  Adr:  76.99% (  71449/  92804)  Res:  69.59% (  64579/  92804)
	    2  Both:  51.90% (  47657/  91822)  Adr:  73.73% (  67700/  91822)  Res:  69.62% (  63927/  91822)
	    3  Both:  51.21% (  38088/  74378)  Adr:  72.67% (  54049/  74378)  Res:  69.43% (  51637/  74378)
	    4  Both:  49.94% (  38982/  78053)  Adr:  70.41% (  54956/  78053)  Res:  70.02% (  54653/  78053)
	    5  Both:  48.47% (  46811/  96569)  Adr:  64.53% (  62316/  96569)  Res:  73.54% (  71018/  96569)
	    6  Both:  43.93% (  85371/ 194354)  Adr:  57.31% ( 111376/ 194354)  Res:  74.80% ( 145372/ 194354)


  DEV    100/1457  200/1457  300/1457  400/1457  500/1457  600/1457  700/1457  800/1457  900/1457  1000/1457  1100/1457  1200/1457  1300/1457  1400/1457
	Time: 29.850512
	Total Loss: 0.000000	Total Grad Norm: 0.000000
	Avg.  Loss: 0.000000	Avg.  Grad Norm: 0.000000

	Accuracy
	TOTAL  Both:  51.11% (  23068/  45132)  Adr:  66.46% (  29996/  45132)  Res:  75.61% (  34126/  45132)

	    0  Both:  64.00% (   2176/   3400)  Adr:  87.79% (   2985/   3400)  Res:  72.47% (   2464/   3400)
	    1  Both:  56.93% (   3098/   5442)  Adr:  76.85% (   4182/   5442)  Res:  73.01% (   3973/   5442)
	    2  Both:  50.91% (   2691/   5286)  Adr:  68.82% (   3638/   5286)  Res:  73.14% (   3866/   5286)
	    3  Both:  49.73% (   2115/   4253)  Adr:  66.49% (   2828/   4253)  Res:  73.55% (   3128/   4253)
	    4  Both:  50.70% (   2285/   4507)  Adr:  66.74% (   3008/   4507)  Res:  74.15% (   3342/   4507)
	    5  Both:  50.88% (   3502/   6883)  Adr:  63.87% (   4396/   6883)  Res:  77.06% (   5304/   6883)
	    6  Both:  46.88% (   7201/  15361)  Adr:  58.32% (   8959/  15361)  Res:  78.44% (  12049/  15361)


  TEST    100/1669  200/1669  300/1669  400/1669  500/1669  600/1669  700/1669  800/1669  900/1669  1000/1669  1100/1669  1200/1669  1300/1669  1400/1669  1500/1669  1600/1669
	Time: 34.482447
	Total Loss: 0.000000	Total Grad Norm: 0.000000
	Avg.  Loss: 0.000000	Avg.  Grad Norm: 0.000000

	Accuracy
	TOTAL  Both:  51.53% (  26745/  51897)  Adr:  66.85% (  34691/  51897)  Res:  75.54% (  39204/  51897)

	    0  Both:  64.49% (   2406/   3731)  Adr:  87.24% (   3255/   3731)  Res:  73.12% (   2728/   3731)
	    1  Both:  55.52% (   3310/   5962)  Adr:  76.40% (   4555/   5962)  Res:  71.65% (   4272/   5962)
	    2  Both:  51.76% (   2834/   5475)  Adr:  68.82% (   3768/   5475)  Res:  73.22% (   4009/   5475)
	    3  Both:  53.04% (   2384/   4495)  Adr:  71.66% (   3221/   4495)  Res:  72.41% (   3255/   4495)
	    4  Both:  49.55% (   2784/   5619)  Adr:  65.95% (   3706/   5619)  Res:  73.39% (   4124/   5619)
	    5  Both:  51.82% (   4123/   7956)  Adr:  64.69% (   5147/   7956)  Res:  77.99% (   6205/   7956)
	    6  Both:  47.72% (   8904/  18659)  Adr:  59.16% (  11039/  18659)  Res:  78.31% (  14611/  18659)

	BEST ACCURACY HISTORY
	EPOCH-  1 | DEV  Both: 51.11%  Adr: 66.46%  Res: 75.61% | TEST  Both: 51.53%  Adr: 66.85%  Res: 75.54%


Epoch: 2
  TRAIN    100/20798  200/20798  300/20798  400/20798  500/20798  600/20798  700/20798  800/20798  900/20798  1000/20798  1100/20798  1200/20798  1300/20798  1400/20798  1500/20798  1600/20798  1700/20798  1800/20798  1900/20798  2000/20798  2100/20798  2200/20798  2300/20798  2400/20798  2500/20798  2600/20798  2700/20798  2800/20798  2900/20798  3000/20798  3100/20798  3200/20798  3300/20798  3400/20798  3500/20798  3600/20798  3700/20798  3800/20798  3900/20798  4000/20798  4100/20798  4200/20798  4300/20798  4400/20798  4500/20798  4600/20798  4700/20798  4800/20798  4900/20798  5000/20798  5100/20798  5200/20798  5300/20798  5400/20798  5500/20798  5600/20798  5700/20798  5800/20798  5900/20798  6000/20798  6100/20798  6200/20798  6300/20798  6400/20798  6500/20798  6600/20798  6700/20798  6800/20798  6900/20798  7000/20798  7100/20798  7200/20798  7300/20798  7400/20798  7500/20798  7600/20798  7700/20798  7800/20798  7900/20798  8000/20798  8100/20798  8200/20798  8300/20798  8400/20798  8500/20798  8600/20798  8700/20798  8800/20798  8900/20798  9000/20798  9100/20798  9200/20798  9300/20798  9400/20798  9500/20798  9600/20798  9700/20798  9800/20798  9900/20798  10000/20798  10100/20798  10200/20798  10300/20798  10400/20798  10500/20798  10600/20798  10700/20798  10800/20798  10900/20798  11000/20798  11100/20798  11200/20798  11300/20798  11400/20798  11500/20798  11600/20798  11700/20798  11800/20798  11900/20798  12000/20798  12100/20798  12200/20798  12300/20798  12400/20798  12500/20798  12600/20798  12700/20798  12800/20798  12900/20798  13000/20798  13100/20798  13200/20798  13300/20798  13400/20798  13500/20798  13600/20798  13700/20798  13800/20798  13900/20798  14000/20798  14100/20798  14200/20798  14300/20798  14400/20798  14500/20798  14600/20798  14700/20798  14800/20798  14900/20798  15000/20798  15100/20798  15200/20798  15300/20798  15400/20798  15500/20798  15600/20798  15700/20798  15800/20798  15900/20798  16000/20798  16100/20798  16200/20798  16300/20798  16400/20798  16500/20798  16600/20798  16700/20798  16800/20798  16900/20798  17000/20798  17100/20798  17200/20798  17300/20798  17400/20798  17500/20798  17600/20798  17700/20798  17800/20798  17900/20798  18000/20798  18100/20798  18200/20798  18300/20798  18400/20798  18500/20798  18600/20798  18700/20798  18800/20798  18900/20798  19000/20798  19100/20798  19200/20798  19300/20798  19400/20798  19500/20798  19600/20798  19700/20798  19800/20798  19900/20798  20000/20798  20100/20798  20200/20798  20300/20798  20400/20798  20500/20798  20600/20798  20700/20798
	Time: 1444.623419
	Total Loss: 706948.990741	Total Grad Norm: 161467.913596
	Avg.  Loss: 33.991201	Avg.  Grad Norm: 7.763627

	Accuracy
	TOTAL  Both:  55.57% ( 369818/ 665536)  Adr:  70.91% ( 471903/ 665536)  Res:  77.18% ( 513632/ 665536)

	    0  Both:  64.81% (  24340/  37556)  Adr:  86.79% (  32593/  37556)  Res:  74.24% (  27881/  37556)
	    1  Both:  59.27% (  55001/  92804)  Adr:  78.25% (  72617/  92804)  Res:  74.79% (  69409/  92804)
	    2  Both:  57.26% (  52581/  91822)  Adr:  75.39% (  69221/  91822)  Res:  74.85% (  68731/  91822)
	    3  Both:  56.62% (  42112/  74378)  Adr:  74.47% (  55386/  74378)  Res:  74.76% (  55604/  74378)
	    4  Both:  55.50% (  43320/  78053)  Adr:  72.28% (  56417/  78053)  Res:  75.56% (  58978/  78053)
	    5  Both:  55.06% (  53167/  96569)  Adr:  67.81% (  65485/  96569)  Res:  79.38% (  76654/  96569)
	    6  Both:  51.09% (  99297/ 194354)  Adr:  61.84% ( 120184/ 194354)  Res:  80.46% ( 156375/ 194354)


  DEV    100/1457  200/1457  300/1457  400/1457  500/1457  600/1457  700/1457  800/1457  900/1457  1000/1457  1100/1457  1200/1457  1300/1457  1400/1457
	Time: 29.007221
	Total Loss: 0.000000	Total Grad Norm: 0.000000
	Avg.  Loss: 0.000000	Avg.  Grad Norm: 0.000000

	Accuracy
	TOTAL  Both:  53.54% (  24164/  45132)  Adr:  67.64% (  30526/  45132)  Res:  77.76% (  35093/  45132)

	    0  Both:  66.18% (   2250/   3400)  Adr:  88.29% (   3002/   3400)  Res:  74.32% (   2527/   3400)
	    1  Both:  59.19% (   3221/   5442)  Adr:  77.10% (   4196/   5442)  Res:  75.56% (   4112/   5442)
	    2  Both:  53.54% (   2830/   5286)  Adr:  69.66% (   3682/   5286)  Res:  75.10% (   3970/   5286)
	    3  Both:  51.05% (   2171/   4253)  Adr:  66.99% (   2849/   4253)  Res:  75.10% (   3194/   4253)
	    4  Both:  52.39% (   2361/   4507)  Adr:  67.27% (   3032/   4507)  Res:  75.95% (   3423/   4507)
	    5  Both:  53.52% (   3684/   6883)  Adr:  65.52% (   4510/   6883)  Res:  79.54% (   5475/   6883)
	    6  Both:  49.78% (   7647/  15361)  Adr:  60.25% (   9255/  15361)  Res:  80.67% (  12392/  15361)


  TEST    100/1669  200/1669  300/1669  400/1669  500/1669  600/1669  700/1669  800/1669  900/1669  1000/1669  1100/1669  1200/1669  1300/1669  1400/1669  1500/1669  1600/1669
	Time: 33.584712
	Total Loss: 0.000000	Total Grad Norm: 0.000000
	Avg.  Loss: 0.000000	Avg.  Grad Norm: 0.000000

	Accuracy
	TOTAL  Both:  54.28% (  28170/  51897)  Adr:  68.02% (  35300/  51897)  Res:  78.20% (  40585/  51897)

	    0  Both:  67.25% (   2509/   3731)  Adr:  87.38% (   3260/   3731)  Res:  76.25% (   2845/   3731)
	    1  Both:  57.95% (   3455/   5962)  Adr:  76.59% (   4566/   5962)  Res:  74.49% (   4441/   5962)
	    2  Both:  54.41% (   2979/   5475)  Adr:  70.03% (   3834/   5475)  Res:  75.32% (   4124/   5475)
	    3  Both:  55.51% (   2495/   4495)  Adr:  72.39% (   3254/   4495)  Res:  74.84% (   3364/   4495)
	    4  Both:  51.56% (   2897/   5619)  Adr:  66.88% (   3758/   5619)  Res:  75.58% (   4247/   5619)
	    5  Both:  54.75% (   4356/   7956)  Adr:  65.74% (   5230/   7956)  Res:  80.93% (   6439/   7956)
	    6  Both:  50.80% (   9479/  18659)  Adr:  61.09% (  11398/  18659)  Res:  81.06% (  15125/  18659)

	BEST ACCURACY HISTORY
	EPOCH-  1 | DEV  Both: 51.11%  Adr: 66.46%  Res: 75.61% | TEST  Both: 51.53%  Adr: 66.85%  Res: 75.54%
	EPOCH-  2 | DEV  Both: 53.54%  Adr: 67.64%  Res: 77.76% | TEST  Both: 54.28%  Adr: 68.02%  Res: 78.20%


Epoch: 3
  TRAIN    100/20798  200/20798  300/20798  400/20798  500/20798  600/20798  700/20798  800/20798  900/20798  1000/20798  1100/20798  1200/20798  1300/20798  1400/20798  1500/20798  1600/20798  1700/20798  1800/20798  1900/20798  2000/20798  2100/20798  2200/20798  2300/20798  2400/20798  2500/20798  2600/20798  2700/20798  2800/20798  2900/20798  3000/20798  3100/20798  3200/20798  3300/20798  3400/20798  3500/20798  3600/20798  3700/20798  3800/20798  3900/20798  4000/20798  4100/20798  4200/20798  4300/20798  4400/20798  4500/20798  4600/20798  4700/20798  4800/20798  4900/20798  5000/20798  5100/20798  5200/20798  5300/20798  5400/20798  5500/20798  5600/20798  5700/20798  5800/20798  5900/20798  6000/20798  6100/20798  6200/20798  6300/20798  6400/20798  6500/20798  6600/20798  6700/20798  6800/20798  6900/20798  7000/20798  7100/20798  7200/20798  7300/20798  7400/20798  7500/20798  7600/20798  7700/20798  7800/20798  7900/20798  8000/20798  8100/20798  8200/20798  8300/20798  8400/20798  8500/20798  8600/20798  8700/20798  8800/20798  8900/20798  9000/20798  9100/20798  9200/20798  9300/20798  9400/20798  9500/20798  9600/20798  9700/20798  9800/20798  9900/20798  10000/20798  10100/20798  10200/20798  10300/20798  10400/20798  10500/20798  10600/20798  10700/20798  10800/20798  10900/20798  11000/20798  11100/20798  11200/20798  11300/20798  11400/20798  11500/20798  11600/20798  11700/20798  11800/20798  11900/20798  12000/20798  12100/20798  12200/20798  12300/20798  12400/20798  12500/20798  12600/20798  12700/20798  12800/20798  12900/20798  13000/20798  13100/20798  13200/20798  13300/20798  13400/20798  13500/20798  13600/20798  13700/20798  13800/20798  13900/20798  14000/20798  14100/20798  14200/20798  14300/20798  14400/20798  14500/20798  14600/20798  14700/20798  14800/20798  14900/20798  15000/20798  15100/20798  15200/20798  15300/20798  15400/20798  15500/20798  15600/20798  15700/20798  15800/20798  15900/20798  16000/20798  16100/20798  16200/20798  16300/20798  16400/20798  16500/20798  16600/20798  16700/20798  16800/20798  16900/20798  17000/20798  17100/20798  17200/20798  17300/20798  17400/20798  17500/20798  17600/20798  17700/20798  17800/20798  17900/20798  18000/20798  18100/20798  18200/20798  18300/20798  18400/20798  18500/20798  18600/20798  18700/20798  18800/20798  18900/20798  19000/20798  19100/20798  19200/20798  19300/20798  19400/20798  19500/20798  19600/20798  19700/20798  19800/20798  19900/20798  20000/20798  20100/20798  20200/20798  20300/20798  20400/20798  20500/20798  20600/20798  20700/20798
	Time: 1428.588088
	Total Loss: 681622.885879	Total Grad Norm: 173783.851473
	Avg.  Loss: 32.773482	Avg.  Grad Norm: 8.355796

	Accuracy
	TOTAL  Both:  58.51% ( 389418/ 665536)  Adr:  72.15% ( 480169/ 665536)  Res:  79.80% ( 531117/ 665536)

	    0  Both:  67.58% (  25380/  37556)  Adr:  87.12% (  32719/  37556)  Res:  76.97% (  28907/  37556)
	    1  Both:  61.69% (  57247/  92804)  Adr:  78.95% (  73265/  92804)  Res:  77.15% (  71596/  92804)
	    2  Both:  59.96% (  55057/  91822)  Adr:  76.07% (  69849/  91822)  Res:  77.55% (  71212/  91822)
	    3  Both:  59.37% (  44159/  74378)  Adr:  75.36% (  56050/  74378)  Res:  77.52% (  57660/  74378)
	    4  Both:  58.20% (  45430/  78053)  Adr:  73.41% (  57301/  78053)  Res:  77.97% (  60861/  78053)
	    5  Both:  58.16% (  56162/  96569)  Adr:  69.28% (  66903/  96569)  Res:  82.02% (  79208/  96569)
	    6  Both:  54.53% ( 105983/ 194354)  Adr:  63.84% ( 124082/ 194354)  Res:  83.18% ( 161673/ 194354)


  DEV    100/1457  200/1457  300/1457  400/1457  500/1457  600/1457  700/1457  800/1457  900/1457  1000/1457  1100/1457  1200/1457  1300/1457  1400/1457
	Time: 28.769084
	Total Loss: 0.000000	Total Grad Norm: 0.000000
	Avg.  Loss: 0.000000	Avg.  Grad Norm: 0.000000

	Accuracy
	TOTAL  Both:  54.53% (  24611/  45132)  Adr:  67.95% (  30668/  45132)  Res:  78.75% (  35540/  45132)

	    0  Both:  66.82% (   2272/   3400)  Adr:  88.15% (   2997/   3400)  Res:  74.94% (   2548/   3400)
	    1  Both:  59.94% (   3262/   5442)  Adr:  76.99% (   4190/   5442)  Res:  76.64% (   4171/   5442)
	    2  Both:  54.22% (   2866/   5286)  Adr:  69.66% (   3682/   5286)  Res:  75.56% (   3994/   5286)
	    3  Both:  51.99% (   2211/   4253)  Adr:  67.01% (   2850/   4253)  Res:  76.02% (   3233/   4253)
	    4  Both:  53.65% (   2418/   4507)  Adr:  67.74% (   3053/   4507)  Res:  77.01% (   3471/   4507)
	    5  Both:  55.06% (   3790/   6883)  Adr:  66.41% (   4571/   6883)  Res:  80.84% (   5564/   6883)
	    6  Both:  50.73% (   7792/  15361)  Adr:  60.71% (   9325/  15361)  Res:  81.76% (  12559/  15361)


  TEST    100/1669  200/1669  300/1669  400/1669  500/1669  600/1669  700/1669  800/1669  900/1669  1000/1669  1100/1669  1200/1669  1300/1669  1400/1669  1500/1669  1600/1669
	Time: 32.867068
	Total Loss: 0.000000	Total Grad Norm: 0.000000
	Avg.  Loss: 0.000000	Avg.  Grad Norm: 0.000000

	Accuracy
	TOTAL  Both:  55.54% (  28822/  51897)  Adr:  68.72% (  35665/  51897)  Res:  79.16% (  41081/  51897)

	    0  Both:  67.25% (   2509/   3731)  Adr:  87.48% (   3264/   3731)  Res:  75.93% (   2833/   3731)
	    1  Both:  58.45% (   3485/   5962)  Adr:  76.74% (   4575/   5962)  Res:  75.14% (   4480/   5962)
	    2  Both:  55.20% (   3022/   5475)  Adr:  70.16% (   3841/   5475)  Res:  76.29% (   4177/   5475)
	    3  Both:  56.84% (   2555/   4495)  Adr:  72.61% (   3264/   4495)  Res:  76.28% (   3429/   4495)
	    4  Both:  52.63% (   2957/   5619)  Adr:  67.20% (   3776/   5619)  Res:  76.22% (   4283/   5619)
	    5  Both:  55.62% (   4425/   7956)  Adr:  66.50% (   5291/   7956)  Res:  81.60% (   6492/   7956)
	    6  Both:  52.89% (   9869/  18659)  Adr:  62.46% (  11654/  18659)  Res:  82.46% (  15387/  18659)

	BEST ACCURACY HISTORY
	EPOCH-  1 | DEV  Both: 51.11%  Adr: 66.46%  Res: 75.61% | TEST  Both: 51.53%  Adr: 66.85%  Res: 75.54%
	EPOCH-  2 | DEV  Both: 53.54%  Adr: 67.64%  Res: 77.76% | TEST  Both: 54.28%  Adr: 68.02%  Res: 78.20%
	EPOCH-  3 | DEV  Both: 54.53%  Adr: 67.95%  Res: 78.75% | TEST  Both: 55.54%  Adr: 68.72%  Res: 79.16%


Epoch: 4
  TRAIN    100/20798  200/20798  300/20798  400/20798  500/20798  600/20798  700/20798  800/20798  900/20798  1000/20798  1100/20798  1200/20798  1300/20798  1400/20798  1500/20798  1600/20798  1700/20798  1800/20798  1900/20798  2000/20798  2100/20798  2200/20798  2300/20798  2400/20798  2500/20798  2600/20798  2700/20798  2800/20798  2900/20798  3000/20798  3100/20798  3200/20798  3300/20798  3400/20798  3500/20798  3600/20798  3700/20798  3800/20798  3900/20798  4000/20798  4100/20798  4200/20798  4300/20798  4400/20798  4500/20798  4600/20798  4700/20798  4800/20798  4900/20798  5000/20798  5100/20798  5200/20798  5300/20798  5400/20798  5500/20798  5600/20798  5700/20798  5800/20798  5900/20798  6000/20798  6100/20798  6200/20798  6300/20798  6400/20798  6500/20798  6600/20798  6700/20798  6800/20798  6900/20798  7000/20798  7100/20798  7200/20798  7300/20798  7400/20798  7500/20798  7600/20798  7700/20798  7800/20798  7900/20798  8000/20798  8100/20798  8200/20798  8300/20798  8400/20798  8500/20798  8600/20798  8700/20798  8800/20798  8900/20798  9000/20798  9100/20798  9200/20798  9300/20798  9400/20798  9500/20798  9600/20798  9700/20798  9800/20798  9900/20798  10000/20798  10100/20798  10200/20798  10300/20798  10400/20798  10500/20798  10600/20798  10700/20798  10800/20798  10900/20798  11000/20798  11100/20798  11200/20798  11300/20798  11400/20798  11500/20798  11600/20798  11700/20798  11800/20798  11900/20798  12000/20798  12100/20798  12200/20798  12300/20798  12400/20798  12500/20798  12600/20798  12700/20798  12800/20798  12900/20798  13000/20798  13100/20798  13200/20798  13300/20798  13400/20798  13500/20798  13600/20798  13700/20798  13800/20798  13900/20798  14000/20798  14100/20798  14200/20798  14300/20798  14400/20798  14500/20798  14600/20798  14700/20798  14800/20798  14900/20798  15000/20798  15100/20798  15200/20798  15300/20798  15400/20798  15500/20798  15600/20798  15700/20798  15800/20798  15900/20798  16000/20798  16100/20798  16200/20798  16300/20798  16400/20798  16500/20798  16600/20798  16700/20798  16800/20798  16900/20798  17000/20798  17100/20798  17200/20798  17300/20798  17400/20798  17500/20798  17600/20798  17700/20798  17800/20798  17900/20798  18000/20798  18100/20798  18200/20798  18300/20798  18400/20798  18500/20798  18600/20798  18700/20798  18800/20798  18900/20798  19000/20798  19100/20798  19200/20798  19300/20798  19400/20798  19500/20798  19600/20798  19700/20798  19800/20798  19900/20798  20000/20798  20100/20798  20200/20798  20300/20798  20400/20798  20500/20798  20600/20798  20700/20798
	Time: 1382.305269
	Total Loss: 658841.707127	Total Grad Norm: 189789.832917
	Avg.  Loss: 31.678128	Avg.  Grad Norm: 9.125389

	Accuracy
	TOTAL  Both:  60.86% ( 405015/ 665536)  Adr:  73.16% ( 486890/ 665536)  Res:  81.90% ( 545086/ 665536)

	    0  Both:  69.83% (  26226/  37556)  Adr:  87.58% (  32891/  37556)  Res:  79.20% (  29745/  37556)
	    1  Both:  64.11% (  59494/  92804)  Adr:  79.75% (  74010/  92804)  Res:  79.35% (  73640/  92804)
	    2  Both:  62.34% (  57238/  91822)  Adr:  76.93% (  70639/  91822)  Res:  79.82% (  73289/  91822)
	    3  Both:  61.80% (  45963/  74378)  Adr:  76.21% (  56681/  74378)  Res:  79.79% (  59346/  74378)
	    4  Both:  60.58% (  47281/  78053)  Adr:  74.17% (  57891/  78053)  Res:  80.29% (  62670/  78053)
	    5  Both:  60.42% (  58347/  96569)  Adr:  70.41% (  67994/  96569)  Res:  84.04% (  81161/  96569)
	    6  Both:  56.84% ( 110466/ 194354)  Adr:  65.23% ( 126784/ 194354)  Res:  85.02% ( 165235/ 194354)


  DEV    100/1457  200/1457  300/1457  400/1457  500/1457  600/1457  700/1457  800/1457  900/1457  1000/1457  1100/1457  1200/1457  1300/1457  1400/1457
	Time: 27.924981
	Total Loss: 0.000000	Total Grad Norm: 0.000000
	Avg.  Loss: 0.000000	Avg.  Grad Norm: 0.000000

	Accuracy
	TOTAL  Both:  54.90% (  24777/  45132)  Adr:  68.51% (  30918/  45132)  Res:  78.59% (  35471/  45132)

	    0  Both:  65.50% (   2227/   3400)  Adr:  88.12% (   2996/   3400)  Res:  73.97% (   2515/   3400)
	    1  Both:  59.28% (   3226/   5442)  Adr:  77.20% (   4201/   5442)  Res:  75.41% (   4104/   5442)
	    2  Both:  54.52% (   2882/   5286)  Adr:  70.37% (   3720/   5286)  Res:  75.44% (   3988/   5286)
	    3  Both:  52.41% (   2229/   4253)  Adr:  67.22% (   2859/   4253)  Res:  76.09% (   3236/   4253)
	    4  Both:  54.14% (   2440/   4507)  Adr:  68.32% (   3079/   4507)  Res:  77.35% (   3486/   4507)
	    5  Both:  55.46% (   3817/   6883)  Adr:  66.74% (   4594/   6883)  Res:  80.69% (   5554/   6883)
	    6  Both:  51.79% (   7956/  15361)  Adr:  61.64% (   9469/  15361)  Res:  81.95% (  12588/  15361)


  TEST    100/1669  200/1669  300/1669  400/1669  500/1669  600/1669  700/1669  800/1669  900/1669  1000/1669  1100/1669  1200/1669  1300/1669  1400/1669  1500/1669  1600/1669
	Time: 32.003315
	Total Loss: 0.000000	Total Grad Norm: 0.000000
	Avg.  Loss: 0.000000	Avg.  Grad Norm: 0.000000

	Accuracy
	TOTAL  Both:  55.53% (  28816/  51897)  Adr:  68.80% (  35706/  51897)  Res:  79.08% (  41039/  51897)

	    0  Both:  68.08% (   2540/   3731)  Adr:  87.32% (   3258/   3731)  Res:  77.24% (   2882/   3731)
	    1  Both:  58.40% (   3482/   5962)  Adr:  76.74% (   4575/   5962)  Res:  74.74% (   4456/   5962)
	    2  Both:  55.18% (   3021/   5475)  Adr:  70.08% (   3837/   5475)  Res:  76.00% (   4161/   5475)
	    3  Both:  56.24% (   2528/   4495)  Adr:  72.32% (   3251/   4495)  Res:  76.31% (   3430/   4495)
	    4  Both:  52.46% (   2948/   5619)  Adr:  67.25% (   3779/   5619)  Res:  76.08% (   4275/   5619)
	    5  Both:  55.86% (   4444/   7956)  Adr:  66.78% (   5313/   7956)  Res:  81.83% (   6510/   7956)
	    6  Both:  52.81% (   9853/  18659)  Adr:  62.67% (  11693/  18659)  Res:  82.13% (  15325/  18659)

	BEST ACCURACY HISTORY
	EPOCH-  1 | DEV  Both: 51.11%  Adr: 66.46%  Res: 75.61% | TEST  Both: 51.53%  Adr: 66.85%  Res: 75.54%
	EPOCH-  2 | DEV  Both: 53.54%  Adr: 67.64%  Res: 77.76% | TEST  Both: 54.28%  Adr: 68.02%  Res: 78.20%
	EPOCH-  3 | DEV  Both: 54.53%  Adr: 67.95%  Res: 78.75% | TEST  Both: 55.54%  Adr: 68.72%  Res: 79.16%
	EPOCH-  4 | DEV  Both: 54.90%  Adr: 68.51%  Res: 78.59% | TEST  Both: 55.53%  Adr: 68.80%  Res: 79.08%


Epoch: 5
  TRAIN    100/20798  200/20798  300/20798  400/20798  500/20798  600/20798  700/20798  800/20798  900/20798  1000/20798  1100/20798  1200/20798  1300/20798  1400/20798  1500/20798  1600/20798  1700/20798  1800/20798  1900/20798  2000/20798  2100/20798  2200/20798  2300/20798  2400/20798  2500/20798  2600/20798  2700/20798  2800/20798  2900/20798  3000/20798  3100/20798  3200/20798  3300/20798  3400/20798  3500/20798  3600/20798  3700/20798  3800/20798  3900/20798  4000/20798  4100/20798  4200/20798  4300/20798  4400/20798  4500/20798  4600/20798  4700/20798  4800/20798  4900/20798  5000/20798  5100/20798  5200/20798  5300/20798  5400/20798  5500/20798  5600/20798  5700/20798  5800/20798  5900/20798  6000/20798  6100/20798  6200/20798  6300/20798  6400/20798  6500/20798  6600/20798  6700/20798  6800/20798  6900/20798  7000/20798  7100/20798  7200/20798  7300/20798  7400/20798  7500/20798  7600/20798  7700/20798  7800/20798  7900/20798  8000/20798  8100/20798  8200/20798  8300/20798  8400/20798  8500/20798  8600/20798  8700/20798  8800/20798  8900/20798  9000/20798  9100/20798  9200/20798  9300/20798  9400/20798  9500/20798  9600/20798  9700/20798  9800/20798  9900/20798  10000/20798  10100/20798  10200/20798  10300/20798  10400/20798  10500/20798  10600/20798  10700/20798  10800/20798  10900/20798  11000/20798  11100/20798  11200/20798  11300/20798  11400/20798  11500/20798  11600/20798  11700/20798  11800/20798  11900/20798  12000/20798  12100/20798  12200/20798  12300/20798  12400/20798  12500/20798  12600/20798  12700/20798  12800/20798  12900/20798  13000/20798  13100/20798  13200/20798  13300/20798  13400/20798  13500/20798  13600/20798  13700/20798  13800/20798  13900/20798  14000/20798  14100/20798  14200/20798  14300/20798  14400/20798  14500/20798  14600/20798  14700/20798  14800/20798  14900/20798  15000/20798  15100/20798  15200/20798  15300/20798  15400/20798  15500/20798  15600/20798  15700/20798  15800/20798  15900/20798  16000/20798  16100/20798  16200/20798  16300/20798  16400/20798  16500/20798  16600/20798  16700/20798  16800/20798  16900/20798  17000/20798  17100/20798  17200/20798  17300/20798  17400/20798  17500/20798  17600/20798  17700/20798  17800/20798  17900/20798  18000/20798  18100/20798  18200/20798  18300/20798  18400/20798  18500/20798  18600/20798  18700/20798  18800/20798  18900/20798  19000/20798  19100/20798  19200/20798  19300/20798  19400/20798  19500/20798  19600/20798  19700/20798  19800/20798  19900/20798  20000/20798  20100/20798  20200/20798  20300/20798  20400/20798  20500/20798  20600/20798  20700/20798
	Time: 1327.006252
	Total Loss: 635876.733743	Total Grad Norm: 211262.228052
	Avg.  Loss: 30.573937	Avg.  Grad Norm: 10.157815

	Accuracy
	TOTAL  Both:  62.97% ( 419069/ 665536)  Adr:  74.12% ( 493275/ 665536)  Res:  83.72% ( 557171/ 665536)

	    0  Both:  72.01% (  27045/  37556)  Adr:  88.09% (  33084/  37556)  Res:  81.25% (  30514/  37556)
	    1  Both:  66.27% (  61497/  92804)  Adr:  80.48% (  74689/  92804)  Res:  81.34% (  75484/  92804)
	    2  Both:  64.33% (  59069/  91822)  Adr:  77.72% (  71363/  91822)  Res:  81.60% (  74927/  91822)
	    3  Both:  64.04% (  47631/  74378)  Adr:  77.05% (  57308/  74378)  Res:  81.85% (  60877/  74378)
	    4  Both:  63.03% (  49198/  78053)  Adr:  75.25% (  58735/  78053)  Res:  82.38% (  64297/  78053)
	    5  Both:  62.42% (  60283/  96569)  Adr:  71.46% (  69006/  96569)  Res:  85.75% (  82805/  96569)
	    6  Both:  58.83% ( 114346/ 194354)  Adr:  66.42% ( 129090/ 194354)  Res:  86.58% ( 168267/ 194354)


  DEV    100/1457  200/1457  300/1457  400/1457  500/1457  600/1457  700/1457  800/1457  900/1457  1000/1457  1100/1457  1200/1457  1300/1457  1400/1457
	Time: 27.881017
	Total Loss: 0.000000	Total Grad Norm: 0.000000
	Avg.  Loss: 0.000000	Avg.  Grad Norm: 0.000000

	Accuracy
	TOTAL  Both:  54.90% (  24779/  45132)  Adr:  68.28% (  30818/  45132)  Res:  78.73% (  35534/  45132)

	    0  Both:  67.06% (   2280/   3400)  Adr:  87.94% (   2990/   3400)  Res:  75.47% (   2566/   3400)
	    1  Both:  59.87% (   3258/   5442)  Adr:  77.03% (   4192/   5442)  Res:  76.15% (   4144/   5442)
	    2  Both:  52.99% (   2801/   5286)  Adr:  70.03% (   3702/   5286)  Res:  74.23% (   3924/   5286)
	    3  Both:  51.82% (   2204/   4253)  Adr:  66.40% (   2824/   4253)  Res:  76.06% (   3235/   4253)
	    4  Both:  54.16% (   2441/   4507)  Adr:  68.07% (   3068/   4507)  Res:  77.44% (   3490/   4507)
	    5  Both:  55.60% (   3827/   6883)  Adr:  66.77% (   4596/   6883)  Res:  80.94% (   5571/   6883)
	    6  Both:  51.87% (   7968/  15361)  Adr:  61.49% (   9446/  15361)  Res:  82.05% (  12604/  15361)


  TEST    100/1669  200/1669  300/1669  400/1669  500/1669  600/1669  700/1669  800/1669  900/1669  1000/1669  1100/1669  1200/1669  1300/1669  1400/1669  1500/1669  1600/1669
	Time: 31.949641
	Total Loss: 0.000000	Total Grad Norm: 0.000000
	Avg.  Loss: 0.000000	Avg.  Grad Norm: 0.000000

	Accuracy
	TOTAL  Both:  55.22% (  28658/  51897)  Adr:  68.24% (  35413/  51897)  Res:  79.19% (  41095/  51897)

	    0  Both:  66.66% (   2487/   3731)  Adr:  87.22% (   3254/   3731)  Res:  75.26% (   2808/   3731)
	    1  Both:  57.65% (   3437/   5962)  Adr:  75.75% (   4516/   5962)  Res:  75.34% (   4492/   5962)
	    2  Both:  55.31% (   3028/   5475)  Adr:  69.88% (   3826/   5475)  Res:  76.40% (   4183/   5475)
	    3  Both:  55.77% (   2507/   4495)  Adr:  72.04% (   3238/   4495)  Res:  75.77% (   3406/   4495)
	    4  Both:  52.27% (   2937/   5619)  Adr:  66.65% (   3745/   5619)  Res:  76.74% (   4312/   5619)
	    5  Both:  56.11% (   4464/   7956)  Adr:  66.42% (   5284/   7956)  Res:  82.24% (   6543/   7956)
	    6  Both:  52.51% (   9798/  18659)  Adr:  61.90% (  11550/  18659)  Res:  82.27% (  15351/  18659)

	BEST ACCURACY HISTORY
	EPOCH-  1 | DEV  Both: 51.11%  Adr: 66.46%  Res: 75.61% | TEST  Both: 51.53%  Adr: 66.85%  Res: 75.54%
	EPOCH-  2 | DEV  Both: 53.54%  Adr: 67.64%  Res: 77.76% | TEST  Both: 54.28%  Adr: 68.02%  Res: 78.20%
	EPOCH-  3 | DEV  Both: 54.53%  Adr: 67.95%  Res: 78.75% | TEST  Both: 55.54%  Adr: 68.72%  Res: 79.16%
	EPOCH-  4 | DEV  Both: 54.90%  Adr: 68.51%  Res: 78.59% | TEST  Both: 55.53%  Adr: 68.80%  Res: 79.08%
	EPOCH-  5 | DEV  Both: 54.90%  Adr: 68.28%  Res: 78.73% | TEST  Both: 55.22%  Adr: 68.24%  Res: 79.19%


Epoch: 6
  TRAIN    100/20798  200/20798  300/20798  400/20798  500/20798  600/20798  700/20798  800/20798  900/20798  1000/20798  1100/20798  1200/20798  1300/20798  1400/20798  1500/20798  1600/20798  1700/20798  1800/20798  1900/20798  2000/20798  2100/20798  2200/20798  2300/20798  2400/20798  2500/20798  2600/20798  2700/20798  2800/20798  2900/20798  3000/20798  3100/20798  3200/20798  3300/20798  3400/20798  3500/20798  3600/20798  3700/20798  3800/20798  3900/20798  4000/20798  4100/20798  4200/20798  4300/20798  4400/20798  4500/20798  4600/20798  4700/20798  4800/20798  4900/20798  5000/20798  5100/20798  5200/20798  5300/20798  5400/20798  5500/20798  5600/20798  5700/20798  5800/20798  5900/20798  6000/20798  6100/20798  6200/20798  6300/20798  6400/20798  6500/20798  6600/20798  6700/20798  6800/20798  6900/20798  7000/20798  7100/20798  7200/20798  7300/20798  7400/20798  7500/20798  7600/20798  7700/20798  7800/20798  7900/20798  8000/20798  8100/20798  8200/20798  8300/20798  8400/20798  8500/20798  8600/20798  8700/20798  8800/20798  8900/20798  9000/20798  9100/20798  9200/20798  9300/20798  9400/20798  9500/20798  9600/20798  9700/20798  9800/20798  9900/20798  10000/20798  10100/20798  10200/20798  10300/20798  10400/20798  10500/20798  10600/20798  10700/20798  10800/20798  10900/20798  11000/20798  11100/20798  11200/20798  11300/20798  11400/20798

Loss is NAN: Mini-Batch Index: 11473
