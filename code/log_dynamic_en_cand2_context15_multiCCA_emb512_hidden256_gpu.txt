WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:
 https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29

Using gpu device 0: GeForce GTX 1070 (CNMeM is disabled, cuDNN 5105)

Namespace(activation='tanh', attention=0, batch=32, data_size=10000000, dev_data='/cl/work/motoki-s/multi_ling_conversation/data/2015_concat/2015_cand2_lang_en_dev.txt', dim_emb=512, dim_hidden=256, emb_type='multi', epoch=30, init_emb='/cl/work/motoki-s/multi_ling_conversation/ouchi/dialog-emnlp2016/DATA-multi/embeddings/fifty_nine.table5.multiCCA.size_512+w_5+it_10.normalized', lang='en', load_param=None, loss='nll', lr=0.001, max_n_words=20, mode='train', model='dynamic', n_cands=2, n_prev_sents=15, opt='adam', output=0, output_fn='dynamic_lang_en_cand2_context15_multiCCA_emb512_hidden256', reg=0.0001, sample_size=1, save=1, test_data='/cl/work/motoki-s/multi_ling_conversation/data/2015_concat/2015_cand2_lang_en_test.txt', train_data='/cl/work/motoki-s/multi_ling_conversation/data/2015_concat/2015_cand2_lang_en_train.txt', unit='gru')


ADDRESSEE AND RESPONSE SELECTION SYSTEM START

SET UP DATASET

Load dataset...
Load initial word embedding...
	Word Embedding Size: 176692

TASK  SETTING
	Response Candidates:2  Contexts:15  Max Word Num:20


Converting words into ids...
	Questions:   751108
	Questions:    39809
	Questions:    45776

Creating samples...
	THREADS:  6606
	  SAMPLES:   678787
	  ADDRESSEE DETECTION CHANCE LEVEL:   1.41%
	  ADDRESSEE DETECTION UPPER BOUND:
		# Cands  1:   0.00% | Total:        0 | Including true-adr:        0 | Not including:        0
		# Cands  2: 100.00% | Total:    43496 | Including true-adr:    43496 | Not including:        0
		# Cands  3: 100.00% | Total:    95499 | Including true-adr:    95499 | Not including:        0
		# Cands  4: 100.00% | Total:   127996 | Including true-adr:   127996 | Not including:        0
		# Cands  5: 100.00% | Total:   132059 | Including true-adr:   132059 | Not including:        0
		# Cands  6: 100.00% | Total:   114414 | Including true-adr:   114414 | Not including:        0
		# Cands  7: 100.00% | Total:    82212 | Including true-adr:    82212 | Not including:        0
		# Cands  8: 100.00% | Total:    48464 | Including true-adr:    48464 | Not including:        0
		# Cands  9: 100.00% | Total:    22810 | Including true-adr:    22810 | Not including:        0
		# Cands 10: 100.00% | Total:     8355 | Including true-adr:     8355 | Not including:        0
		# Cands 11: 100.00% | Total:     2663 | Including true-adr:     2663 | Not including:        0
		# Cands 12: 100.00% | Total:      654 | Including true-adr:      654 | Not including:        0
		# Cands 13: 100.00% | Total:      145 | Including true-adr:      145 | Not including:        0
		# Cands 14: 100.00% | Total:       16 | Including true-adr:       16 | Not including:        0
		# Cands 15: 100.00% | Total:        2 | Including true-adr:        2 | Not including:        0
		# Cands 16: 100.00% | Total:        2 | Including true-adr:        2 | Not including:        0

	  THE BINNED NUMBER OF AGENTS IN CONTEXT:
		Bin  0:    38243
		Bin  1:    93502
		Bin  2:    91568
		Bin  3:    74724
		Bin  4:    78966
		Bin  5:    99521
		Bin  6:   202263

	THREADS:   367
	  SAMPLES:    38661
	  ADDRESSEE DETECTION CHANCE LEVEL:   1.59%
	  ADDRESSEE DETECTION UPPER BOUND:
		# Cands  1:   0.00% | Total:        0 | Including true-adr:        0 | Not including:        0
		# Cands  2:  97.85% | Total:     2934 | Including true-adr:     2871 | Not including:       63
		# Cands  3:  95.86% | Total:     5815 | Including true-adr:     5574 | Not including:      241
		# Cands  4:  96.04% | Total:     7449 | Including true-adr:     7154 | Not including:      295
		# Cands  5:  95.61% | Total:     7045 | Including true-adr:     6736 | Not including:      309
		# Cands  6:  96.27% | Total:     6277 | Including true-adr:     6043 | Not including:      234
		# Cands  7:  96.26% | Total:     4466 | Including true-adr:     4299 | Not including:      167
		# Cands  8:  97.12% | Total:     2743 | Including true-adr:     2664 | Not including:       79
		# Cands  9:  96.67% | Total:     1323 | Including true-adr:     1279 | Not including:       44
		# Cands 10:  95.33% | Total:      450 | Including true-adr:      429 | Not including:       21
		# Cands 11:  98.36% | Total:      122 | Including true-adr:      120 | Not including:        2
		# Cands 12:  96.00% | Total:       25 | Including true-adr:       24 | Not including:        1
		# Cands 13: 100.00% | Total:       12 | Including true-adr:       12 | Not including:        0
		# Cands 14:   0.00% | Total:        0 | Including true-adr:        0 | Not including:        0
		# Cands 15:   0.00% | Total:        0 | Including true-adr:        0 | Not including:        0
		# Cands 16:   0.00% | Total:        0 | Including true-adr:        0 | Not including:        0

	  THE BINNED NUMBER OF AGENTS IN CONTEXT:
		Bin  0:     3092
		Bin  1:     5358
		Bin  2:     5842
		Bin  3:     4048
		Bin  4:     4686
		Bin  5:     5969
		Bin  6:     9666

	THREADS:   382
	  SAMPLES:    44714
	  ADDRESSEE DETECTION CHANCE LEVEL:   1.23%
	  ADDRESSEE DETECTION UPPER BOUND:
		# Cands  1:   0.00% | Total:        3 | Including true-adr:        0 | Not including:        3
		# Cands  2:  97.42% | Total:     2757 | Including true-adr:     2686 | Not including:       71
		# Cands  3:  96.19% | Total:     5591 | Including true-adr:     5378 | Not including:      213
		# Cands  4:  96.14% | Total:     8002 | Including true-adr:     7693 | Not including:      309
		# Cands  5:  96.09% | Total:     8819 | Including true-adr:     8474 | Not including:      345
		# Cands  6:  96.77% | Total:     7856 | Including true-adr:     7602 | Not including:      254
		# Cands  7:  96.72% | Total:     5693 | Including true-adr:     5506 | Not including:      187
		# Cands  8:  96.78% | Total:     3381 | Including true-adr:     3272 | Not including:      109
		# Cands  9:  96.45% | Total:     1690 | Including true-adr:     1630 | Not including:       60
		# Cands 10:  97.59% | Total:      665 | Including true-adr:      649 | Not including:       16
		# Cands 11:  97.42% | Total:      194 | Including true-adr:      189 | Not including:        5
		# Cands 12: 100.00% | Total:       52 | Including true-adr:       52 | Not including:        0
		# Cands 13: 100.00% | Total:        9 | Including true-adr:        9 | Not including:        0
		# Cands 14: 100.00% | Total:        2 | Including true-adr:        2 | Not including:        0
		# Cands 15:   0.00% | Total:        0 | Including true-adr:        0 | Not including:        0
		# Cands 16:   0.00% | Total:        0 | Including true-adr:        0 | Not including:        0

	  THE BINNED NUMBER OF AGENTS IN CONTEXT:
		Bin  0:     3115
		Bin  1:     5387
		Bin  2:     5137
		Bin  3:     4361
		Bin  4:     4455
		Bin  5:     5919
		Bin  6:    16340


TRAIN SETTING	Batch Size:32  Epoch:30  Vocab:176693  Max Words:20

Train samples	Mini-Batch:21206
Dev samples	Mini-Batch:1264
Test samples	Mini-Batch:1453

BUILD A MODEL
MODEL: dynamic  Unit: gru  Opt: adam  Activation: tanh  Parameters: 1245184


TRAINING START



Epoch: 1
  TRAIN    100/21206  200/21206  300/21206  400/21206  500/21206  600/21206  700/21206  800/21206  900/21206  1000/21206  1100/21206  1200/21206  1300/21206  1400/21206  1500/21206  1600/21206  1700/21206  1800/21206  1900/21206  2000/21206  2100/21206  2200/21206  2300/21206  2400/21206  2500/21206  2600/21206  2700/21206  2800/21206  2900/21206  3000/21206  3100/21206  3200/21206  3300/21206  3400/21206  3500/21206  3600/21206  3700/21206  3800/21206  3900/21206  4000/21206  4100/21206  4200/21206  4300/21206  4400/21206  4500/21206  4600/21206  4700/21206  4800/21206  4900/21206  5000/21206  5100/21206  5200/21206  5300/21206  5400/21206  5500/21206  5600/21206  5700/21206  5800/21206  5900/21206  6000/21206  6100/21206  6200/21206  6300/21206  6400/21206  6500/21206  6600/21206  6700/21206  6800/21206  6900/21206  7000/21206  7100/21206  7200/21206  7300/21206  7400/21206  7500/21206  7600/21206  7700/21206  7800/21206  7900/21206  8000/21206  8100/21206  8200/21206  8300/21206  8400/21206  8500/21206  8600/21206  8700/21206  8800/21206  8900/21206  9000/21206  9100/21206  9200/21206  9300/21206  9400/21206  9500/21206  9600/21206  9700/21206  9800/21206  9900/21206  10000/21206  10100/21206  10200/21206  10300/21206  10400/21206  10500/21206  10600/21206  10700/21206  10800/21206  10900/21206  11000/21206  11100/21206  11200/21206  11300/21206  11400/21206  11500/21206  11600/21206  11700/21206  11800/21206  11900/21206  12000/21206  12100/21206  12200/21206  12300/21206  12400/21206  12500/21206  12600/21206  12700/21206  12800/21206  12900/21206  13000/21206  13100/21206  13200/21206  13300/21206  13400/21206  13500/21206  13600/21206  13700/21206  13800/21206  13900/21206  14000/21206  14100/21206  14200/21206  14300/21206  14400/21206  14500/21206  14600/21206  14700/21206  14800/21206  14900/21206  15000/21206  15100/21206  15200/21206  15300/21206  15400/21206  15500/21206  15600/21206  15700/21206  15800/21206  15900/21206  16000/21206  16100/21206  16200/21206  16300/21206  16400/21206  16500/21206  16600/21206  16700/21206  16800/21206  16900/21206  17000/21206  17100/21206  17200/21206  17300/21206  17400/21206  17500/21206  17600/21206  17700/21206  17800/21206  17900/21206  18000/21206  18100/21206  18200/21206  18300/21206  18400/21206  18500/21206  18600/21206  18700/21206  18800/21206  18900/21206  19000/21206  19100/21206  19200/21206  19300/21206  19400/21206  19500/21206  19600/21206  19700/21206  19800/21206  19900/21206  20000/21206  20100/21206  20200/21206  20300/21206  20400/21206  20500/21206  20600/21206  20700/21206  20800/21206  20900/21206  21000/21206  21100/21206  21200/21206
	Time: 1348.337458
	Total Loss: 766108.723308	Total Grad Norm: 151584.000206
	Avg.  Loss: 36.126979	Avg.  Grad Norm: 7.148166

	Accuracy
	TOTAL  Both:  49.54% ( 336167/ 678592)  Adr:  68.11% ( 462218/ 678592)  Res:  71.82% ( 487379/ 678592)

	    0  Both:  60.18% (  23012/  38241)  Adr:  86.20% (  32965/  38241)  Res:  69.41% (  26542/  38241)
	    1  Both:  54.16% (  50636/  93492)  Adr:  77.02% (  72004/  93492)  Res:  69.63% (  65099/  93492)
	    2  Both:  51.78% (  47406/  91551)  Adr:  73.48% (  67270/  91551)  Res:  69.53% (  63658/  91551)
	    3  Both:  51.65% (  38588/  74714)  Adr:  72.83% (  54417/  74714)  Res:  69.87% (  52201/  74714)
	    4  Both:  49.71% (  39248/  78954)  Adr:  70.21% (  55434/  78954)  Res:  69.84% (  55142/  78954)
	    5  Both:  48.68% (  48423/  99481)  Adr:  64.65% (  64312/  99481)  Res:  73.75% (  73364/  99481)
	    6  Both:  43.95% (  88854/ 202159)  Adr:  57.29% ( 115816/ 202159)  Res:  74.88% ( 151373/ 202159)


  DEV    100/1264  200/1264  300/1264  400/1264  500/1264  600/1264  700/1264  800/1264  900/1264  1000/1264  1100/1264  1200/1264
	Time: 23.916541
	Total Loss: 0.000000	Total Grad Norm: 0.000000
	Avg.  Loss: 0.000000	Avg.  Grad Norm: 0.000000

	Accuracy
	TOTAL  Both:  52.46% (  20283/  38661)  Adr:  68.70% (  26560/  38661)  Res:  75.17% (  29063/  38661)

	    0  Both:  64.23% (   1986/   3092)  Adr:  87.55% (   2707/   3092)  Res:  73.29% (   2266/   3092)
	    1  Both:  58.03% (   3109/   5358)  Adr:  78.07% (   4183/   5358)  Res:  73.42% (   3934/   5358)
	    2  Both:  53.42% (   3121/   5842)  Adr:  71.26% (   4163/   5842)  Res:  73.28% (   4281/   5842)
	    3  Both:  50.62% (   2049/   4048)  Adr:  67.54% (   2734/   4048)  Res:  73.79% (   2987/   4048)
	    4  Both:  51.30% (   2404/   4686)  Adr:  68.48% (   3209/   4686)  Res:  73.22% (   3431/   4686)
	    5  Both:  49.82% (   2974/   5969)  Adr:  63.36% (   3782/   5969)  Res:  76.90% (   4590/   5969)
	    6  Both:  48.00% (   4640/   9666)  Adr:  59.82% (   5782/   9666)  Res:  78.36% (   7574/   9666)


  TEST    100/1453  200/1453  300/1453  400/1453  500/1453  600/1453  700/1453  800/1453  900/1453  1000/1453  1100/1453  1200/1453  1300/1453  1400/1453
	Time: 27.612719
	Total Loss: 0.000000	Total Grad Norm: 0.000000
	Avg.  Loss: 0.000000	Avg.  Grad Norm: 0.000000

	Accuracy
	TOTAL  Both:  51.17% (  22882/  44714)  Adr:  66.27% (  29630/  44714)  Res:  75.89% (  33935/  44714)

	    0  Both:  64.56% (   2011/   3115)  Adr:  87.32% (   2720/   3115)  Res:  73.52% (   2290/   3115)
	    1  Both:  56.36% (   3036/   5387)  Adr:  75.22% (   4052/   5387)  Res:  74.48% (   4012/   5387)
	    2  Both:  52.72% (   2708/   5137)  Adr:  71.07% (   3651/   5137)  Res:  72.28% (   3713/   5137)
	    3  Both:  50.61% (   2207/   4361)  Adr:  69.18% (   3017/   4361)  Res:  72.05% (   3142/   4361)
	    4  Both:  49.09% (   2187/   4455)  Adr:  65.59% (   2922/   4455)  Res:  73.11% (   3257/   4455)
	    5  Both:  52.53% (   3109/   5919)  Adr:  65.23% (   3861/   5919)  Res:  78.56% (   4650/   5919)
	    6  Both:  46.66% (   7624/  16340)  Adr:  57.57% (   9407/  16340)  Res:  78.77% (  12871/  16340)

	BEST ACCURACY HISTORY
	EPOCH-  1 | DEV  Both: 52.46%  Adr: 68.70%  Res: 75.17% | TEST  Both: 51.17%  Adr: 66.27%  Res: 75.89%


Epoch: 2
  TRAIN    100/21206  200/21206  300/21206  400/21206  500/21206  600/21206  700/21206  800/21206  900/21206  1000/21206  1100/21206  1200/21206  1300/21206  1400/21206  1500/21206  1600/21206  1700/21206  1800/21206  1900/21206  2000/21206  2100/21206  2200/21206  2300/21206  2400/21206  2500/21206  2600/21206  2700/21206  2800/21206  2900/21206  3000/21206  3100/21206  3200/21206  3300/21206  3400/21206  3500/21206  3600/21206  3700/21206  3800/21206  3900/21206  4000/21206  4100/21206  4200/21206  4300/21206  4400/21206  4500/21206  4600/21206  4700/21206  4800/21206  4900/21206  5000/21206  5100/21206  5200/21206  5300/21206  5400/21206  5500/21206  5600/21206  5700/21206  5800/21206  5900/21206  6000/21206  6100/21206  6200/21206  6300/21206  6400/21206  6500/21206  6600/21206  6700/21206  6800/21206  6900/21206  7000/21206  7100/21206  7200/21206  7300/21206  7400/21206  7500/21206  7600/21206  7700/21206  7800/21206  7900/21206  8000/21206  8100/21206  8200/21206  8300/21206  8400/21206  8500/21206  8600/21206  8700/21206  8800/21206  8900/21206  9000/21206  9100/21206  9200/21206  9300/21206  9400/21206  9500/21206  9600/21206  9700/21206  9800/21206  9900/21206  10000/21206  10100/21206  10200/21206  10300/21206  10400/21206  10500/21206  10600/21206  10700/21206  10800/21206  10900/21206  11000/21206  11100/21206  11200/21206  11300/21206  11400/21206  11500/21206  11600/21206  11700/21206  11800/21206  11900/21206  12000/21206  12100/21206  12200/21206  12300/21206  12400/21206  12500/21206  12600/21206  12700/21206  12800/21206  12900/21206  13000/21206  13100/21206  13200/21206  13300/21206  13400/21206  13500/21206  13600/21206  13700/21206  13800/21206  13900/21206  14000/21206  14100/21206  14200/21206  14300/21206  14400/21206  14500/21206  14600/21206  14700/21206  14800/21206  14900/21206  15000/21206  15100/21206  15200/21206  15300/21206  15400/21206  15500/21206  15600/21206  15700/21206  15800/21206  15900/21206  16000/21206  16100/21206  16200/21206  16300/21206  16400/21206  16500/21206  16600/21206  16700/21206  16800/21206  16900/21206  17000/21206  17100/21206  17200/21206  17300/21206  17400/21206  17500/21206  17600/21206  17700/21206  17800/21206  17900/21206  18000/21206  18100/21206  18200/21206  18300/21206  18400/21206  18500/21206  18600/21206  18700/21206  18800/21206  18900/21206  19000/21206  19100/21206  19200/21206  19300/21206  19400/21206  19500/21206  19600/21206  19700/21206  19800/21206  19900/21206  20000/21206  20100/21206  20200/21206  20300/21206  20400/21206  20500/21206  20600/21206  20700/21206  20800/21206  20900/21206  21000/21206  21100/21206  21200/21206
	Time: 1359.011254
	Total Loss: 719837.850045	Total Grad Norm: 170208.702574
	Avg.  Loss: 33.945008	Avg.  Grad Norm: 8.026441

	Accuracy
	TOTAL  Both:  55.69% ( 377936/ 678592)  Adr:  70.88% ( 480998/ 678592)  Res:  77.36% ( 524979/ 678592)

	    0  Both:  64.88% (  24809/  38241)  Adr:  86.95% (  33251/  38241)  Res:  74.08% (  28330/  38241)
	    1  Both:  59.53% (  55655/  93492)  Adr:  78.34% (  73237/  93492)  Res:  74.97% (  70088/  93492)
	    2  Both:  57.28% (  52442/  91551)  Adr:  75.04% (  68696/  91551)  Res:  75.10% (  68754/  91551)
	    3  Both:  56.77% (  42417/  74714)  Adr:  74.60% (  55733/  74714)  Res:  74.81% (  55890/  74714)
	    4  Both:  55.41% (  43749/  78954)  Adr:  72.16% (  56976/  78954)  Res:  75.38% (  59512/  78954)
	    5  Both:  55.26% (  54976/  99481)  Adr:  67.97% (  67619/  99481)  Res:  79.59% (  79181/  99481)
	    6  Both:  51.39% ( 103888/ 202159)  Adr:  62.07% ( 125486/ 202159)  Res:  80.74% ( 163224/ 202159)


  DEV    100/1264  200/1264  300/1264  400/1264  500/1264  600/1264  700/1264  800/1264  900/1264  1000/1264  1100/1264  1200/1264
	Time: 24.363519
	Total Loss: 0.000000	Total Grad Norm: 0.000000
	Avg.  Loss: 0.000000	Avg.  Grad Norm: 0.000000

	Accuracy
	TOTAL  Both:  54.32% (  21000/  38661)  Adr:  69.44% (  26848/  38661)  Res:  76.87% (  29720/  38661)

	    0  Both:  65.17% (   2015/   3092)  Adr:  87.55% (   2707/   3092)  Res:  73.97% (   2287/   3092)
	    1  Both:  59.18% (   3171/   5358)  Adr:  78.18% (   4189/   5358)  Res:  74.97% (   4017/   5358)
	    2  Both:  54.60% (   3190/   5842)  Adr:  71.65% (   4186/   5842)  Res:  74.82% (   4371/   5842)
	    3  Both:  52.05% (   2107/   4048)  Adr:  68.18% (   2760/   4048)  Res:  75.10% (   3040/   4048)
	    4  Both:  53.39% (   2502/   4686)  Adr:  69.14% (   3240/   4686)  Res:  74.99% (   3514/   4686)
	    5  Both:  53.31% (   3182/   5969)  Adr:  64.95% (   3877/   5969)  Res:  79.49% (   4745/   5969)
	    6  Both:  50.00% (   4833/   9666)  Adr:  60.92% (   5889/   9666)  Res:  80.14% (   7746/   9666)


  TEST    100/1453  200/1453  300/1453  400/1453  500/1453  600/1453  700/1453  800/1453  900/1453  1000/1453  1100/1453  1200/1453  1300/1453  1400/1453
	Time: 28.310452
	Total Loss: 0.000000	Total Grad Norm: 0.000000
	Avg.  Loss: 0.000000	Avg.  Grad Norm: 0.000000

	Accuracy
	TOTAL  Both:  53.53% (  23936/  44714)  Adr:  67.33% (  30107/  44714)  Res:  78.08% (  34914/  44714)

	    0  Both:  66.16% (   2061/   3115)  Adr:  87.61% (   2729/   3115)  Res:  74.83% (   2331/   3115)
	    1  Both:  57.56% (   3101/   5387)  Adr:  75.63% (   4074/   5387)  Res:  75.79% (   4083/   5387)
	    2  Both:  54.97% (   2824/   5137)  Adr:  72.01% (   3699/   5137)  Res:  74.40% (   3822/   5137)
	    3  Both:  52.85% (   2305/   4361)  Adr:  70.01% (   3053/   4361)  Res:  74.68% (   3257/   4361)
	    4  Both:  51.87% (   2311/   4455)  Adr:  66.58% (   2966/   4455)  Res:  76.00% (   3386/   4455)
	    5  Both:  54.55% (   3229/   5919)  Adr:  66.55% (   3939/   5919)  Res:  80.11% (   4742/   5919)
	    6  Both:  49.60% (   8105/  16340)  Adr:  59.04% (   9647/  16340)  Res:  81.35% (  13293/  16340)

	BEST ACCURACY HISTORY
	EPOCH-  1 | DEV  Both: 52.46%  Adr: 68.70%  Res: 75.17% | TEST  Both: 51.17%  Adr: 66.27%  Res: 75.89%
	EPOCH-  2 | DEV  Both: 54.32%  Adr: 69.44%  Res: 76.87% | TEST  Both: 53.53%  Adr: 67.33%  Res: 78.08%


Epoch: 3
  TRAIN    100/21206  200/21206  300/21206  400/21206  500/21206  600/21206  700/21206  800/21206  900/21206  1000/21206  1100/21206  1200/21206  1300/21206  1400/21206  1500/21206  1600/21206  1700/21206  1800/21206  1900/21206  2000/21206  2100/21206  2200/21206  2300/21206  2400/21206  2500/21206  2600/21206  2700/21206  2800/21206  2900/21206  3000/21206  3100/21206  3200/21206  3300/21206  3400/21206  3500/21206  3600/21206  3700/21206  3800/21206  3900/21206  4000/21206  4100/21206  4200/21206  4300/21206  4400/21206  4500/21206  4600/21206  4700/21206  4800/21206  4900/21206  5000/21206  5100/21206  5200/21206  5300/21206  5400/21206  5500/21206  5600/21206  5700/21206  5800/21206  5900/21206  6000/21206  6100/21206  6200/21206  6300/21206  6400/21206  6500/21206  6600/21206  6700/21206  6800/21206  6900/21206  7000/21206  7100/21206  7200/21206  7300/21206  7400/21206  7500/21206  7600/21206  7700/21206  7800/21206  7900/21206  8000/21206  8100/21206  8200/21206  8300/21206  8400/21206  8500/21206  8600/21206  8700/21206  8800/21206  8900/21206  9000/21206  9100/21206  9200/21206  9300/21206  9400/21206  9500/21206  9600/21206  9700/21206  9800/21206  9900/21206  10000/21206  10100/21206  10200/21206  10300/21206  10400/21206  10500/21206  10600/21206  10700/21206  10800/21206  10900/21206  11000/21206  11100/21206  11200/21206  11300/21206  11400/21206  11500/21206  11600/21206  11700/21206  11800/21206  11900/21206  12000/21206  12100/21206  12200/21206  12300/21206  12400/21206  12500/21206  12600/21206  12700/21206  12800/21206  12900/21206  13000/21206  13100/21206  13200/21206  13300/21206  13400/21206  13500/21206  13600/21206  13700/21206  13800/21206  13900/21206  14000/21206  14100/21206  14200/21206  14300/21206  14400/21206  14500/21206  14600/21206  14700/21206  14800/21206  14900/21206  15000/21206  15100/21206  15200/21206  15300/21206  15400/21206  15500/21206  15600/21206  15700/21206  15800/21206  15900/21206  16000/21206  16100/21206  16200/21206  16300/21206  16400/21206  16500/21206  16600/21206  16700/21206  16800/21206  16900/21206  17000/21206  17100/21206  17200/21206  17300/21206  17400/21206  17500/21206  17600/21206  17700/21206  17800/21206  17900/21206  18000/21206  18100/21206  18200/21206  18300/21206  18400/21206  18500/21206  18600/21206  18700/21206  18800/21206  18900/21206  19000/21206  19100/21206  19200/21206  19300/21206  19400/21206  19500/21206  19600/21206  19700/21206  19800/21206  19900/21206  20000/21206  20100/21206  20200/21206  20300/21206  20400/21206  20500/21206  20600/21206  20700/21206  20800/21206  20900/21206  21000/21206  21100/21206  21200/21206
	Time: 1815.042393
	Total Loss: 694849.117430	Total Grad Norm: 180521.698146
	Avg.  Loss: 32.766628	Avg.  Grad Norm: 8.512765

	Accuracy
	TOTAL  Both:  58.44% ( 396542/ 678592)  Adr:  72.03% ( 488767/ 678592)  Res:  79.88% ( 542087/ 678592)

	    0  Both:  67.62% (  25860/  38241)  Adr:  87.19% (  33341/  38241)  Res:  76.97% (  29434/  38241)
	    1  Both:  61.78% (  57760/  93492)  Adr:  79.11% (  73958/  93492)  Res:  77.08% (  72068/  93492)
	    2  Both:  59.67% (  54630/  91551)  Adr:  75.77% (  69370/  91551)  Res:  77.51% (  70960/  91551)
	    3  Both:  59.57% (  44507/  74714)  Adr:  75.45% (  56372/  74714)  Res:  77.62% (  57990/  74714)
	    4  Both:  58.15% (  45915/  78954)  Adr:  73.29% (  57865/  78954)  Res:  77.93% (  61528/  78954)
	    5  Both:  58.24% (  57933/  99481)  Adr:  69.25% (  68888/  99481)  Res:  82.33% (  81899/  99481)
	    6  Both:  54.38% ( 109937/ 202159)  Adr:  63.80% ( 128973/ 202159)  Res:  83.21% ( 168208/ 202159)


  DEV    100/1264  200/1264  300/1264  400/1264  500/1264  600/1264  700/1264  800/1264  900/1264  1000/1264  1100/1264  1200/1264
	Time: 27.151549
	Total Loss: 0.000000	Total Grad Norm: 0.000000
	Avg.  Loss: 0.000000	Avg.  Grad Norm: 0.000000

	Accuracy
	TOTAL  Both:  55.39% (  21416/  38661)  Adr:  69.64% (  26924/  38661)  Res:  77.85% (  30098/  38661)

	    0  Both:  65.75% (   2033/   3092)  Adr:  87.58% (   2708/   3092)  Res:  74.55% (   2305/   3092)
	    1  Both:  60.12% (   3221/   5358)  Adr:  78.39% (   4200/   5358)  Res:  75.25% (   4032/   5358)
	    2  Both:  55.65% (   3251/   5842)  Adr:  71.65% (   4186/   5842)  Res:  76.09% (   4445/   5842)
	    3  Both:  53.24% (   2155/   4048)  Adr:  68.38% (   2768/   4048)  Res:  76.21% (   3085/   4048)
	    4  Both:  54.82% (   2569/   4686)  Adr:  69.31% (   3248/   4686)  Res:  76.18% (   3570/   4686)
	    5  Both:  53.83% (   3213/   5969)  Adr:  65.14% (   3888/   5969)  Res:  79.70% (   4757/   5969)
	    6  Both:  51.46% (   4974/   9666)  Adr:  61.31% (   5926/   9666)  Res:  81.77% (   7904/   9666)


  TEST    100/1453  200/1453  300/1453  400/1453  500/1453  600/1453  700/1453  800/1453  900/1453  1000/1453  1100/1453  1200/1453  1300/1453  1400/1453
	Time: 31.854700
	Total Loss: 0.000000	Total Grad Norm: 0.000000
	Avg.  Loss: 0.000000	Avg.  Grad Norm: 0.000000

	Accuracy
	TOTAL  Both:  54.37% (  24311/  44714)  Adr:  67.91% (  30365/  44714)  Res:  78.46% (  35083/  44714)

	    0  Both:  66.32% (   2066/   3115)  Adr:  87.70% (   2732/   3115)  Res:  75.18% (   2342/   3115)
	    1  Both:  58.07% (   3128/   5387)  Adr:  75.44% (   4064/   5387)  Res:  76.26% (   4108/   5387)
	    2  Both:  55.69% (   2861/   5137)  Adr:  72.08% (   3703/   5137)  Res:  74.85% (   3845/   5137)
	    3  Both:  53.22% (   2321/   4361)  Adr:  69.85% (   3046/   4361)  Res:  74.94% (   3268/   4361)
	    4  Both:  52.59% (   2343/   4455)  Adr:  67.05% (   2987/   4455)  Res:  76.59% (   3412/   4455)
	    5  Both:  56.23% (   3328/   5919)  Adr:  67.51% (   3996/   5919)  Res:  81.09% (   4800/   5919)
	    6  Both:  50.58% (   8264/  16340)  Adr:  60.20% (   9837/  16340)  Res:  81.44% (  13308/  16340)

	BEST ACCURACY HISTORY
	EPOCH-  1 | DEV  Both: 52.46%  Adr: 68.70%  Res: 75.17% | TEST  Both: 51.17%  Adr: 66.27%  Res: 75.89%
	EPOCH-  2 | DEV  Both: 54.32%  Adr: 69.44%  Res: 76.87% | TEST  Both: 53.53%  Adr: 67.33%  Res: 78.08%
	EPOCH-  3 | DEV  Both: 55.39%  Adr: 69.64%  Res: 77.85% | TEST  Both: 54.37%  Adr: 67.91%  Res: 78.46%


Epoch: 4
  TRAIN    100/21206  200/21206  300/21206  400/21206  500/21206  600/21206  700/21206  800/21206  900/21206  1000/21206  1100/21206  1200/21206  1300/21206  1400/21206  1500/21206  1600/21206  1700/21206  1800/21206  1900/21206  2000/21206  2100/21206  2200/21206  2300/21206  2400/21206  2500/21206  2600/21206  2700/21206  2800/21206  2900/21206  3000/21206  3100/21206  3200/21206  3300/21206  3400/21206  3500/21206  3600/21206  3700/21206  3800/21206  3900/21206  4000/21206  4100/21206  4200/21206  4300/21206  4400/21206  4500/21206  4600/21206  4700/21206  4800/21206  4900/21206  5000/21206  5100/21206  5200/21206  5300/21206  5400/21206  5500/21206  5600/21206  5700/21206  5800/21206  5900/21206  6000/21206  6100/21206  6200/21206  6300/21206  6400/21206  6500/21206  6600/21206  6700/21206  6800/21206  6900/21206  7000/21206  7100/21206  7200/21206  7300/21206  7400/21206  7500/21206  7600/21206  7700/21206  7800/21206  7900/21206  8000/21206  8100/21206  8200/21206  8300/21206  8400/21206  8500/21206  8600/21206  8700/21206  8800/21206  8900/21206  9000/21206  9100/21206  9200/21206  9300/21206  9400/21206  9500/21206  9600/21206  9700/21206  9800/21206  9900/21206  10000/21206  10100/21206  10200/21206  10300/21206  10400/21206  10500/21206  10600/21206  10700/21206  10800/21206  10900/21206  11000/21206  11100/21206  11200/21206  11300/21206  11400/21206  11500/21206  11600/21206  11700/21206  11800/21206  11900/21206  12000/21206  12100/21206  12200/21206  12300/21206  12400/21206  12500/21206  12600/21206  12700/21206  12800/21206  12900/21206  13000/21206  13100/21206  13200/21206  13300/21206  13400/21206  13500/21206  13600/21206  13700/21206  13800/21206  13900/21206  14000/21206  14100/21206  14200/21206  14300/21206  14400/21206  14500/21206  14600/21206  14700/21206  14800/21206  14900/21206  15000/21206  15100/21206  15200/21206  15300/21206  15400/21206  15500/21206  15600/21206  15700/21206  15800/21206  15900/21206  16000/21206  16100/21206  16200/21206  16300/21206  16400/21206  16500/21206  16600/21206  16700/21206  16800/21206  16900/21206  17000/21206  17100/21206  17200/21206  17300/21206  17400/21206  17500/21206  17600/21206  17700/21206  17800/21206  17900/21206  18000/21206  18100/21206  18200/21206  18300/21206  18400/21206  18500/21206  18600/21206  18700/21206  18800/21206  18900/21206  19000/21206  19100/21206  19200/21206  19300/21206  19400/21206  19500/21206  19600/21206  19700/21206  19800/21206  19900/21206  20000/21206  20100/21206  20200/21206  20300/21206  20400/21206  20500/21206  20600/21206  20700/21206  20800/21206  20900/21206  21000/21206  21100/21206  21200/21206
	Time: 1562.267305
	Total Loss: 671961.257527	Total Grad Norm: 195850.002988
	Avg.  Loss: 31.687318	Avg.  Grad Norm: 9.235594

	Accuracy
	TOTAL  Both:  60.78% ( 412477/ 678592)  Adr:  73.07% ( 495863/ 678592)  Res:  81.91% ( 555811/ 678592)

	    0  Both:  69.71% (  26657/  38241)  Adr:  87.64% (  33513/  38241)  Res:  78.96% (  30197/  38241)
	    1  Both:  63.90% (  59746/  93492)  Adr:  79.69% (  74504/  93492)  Res:  79.19% (  74039/  93492)
	    2  Both:  61.94% (  56708/  91551)  Adr:  76.61% (  70138/  91551)  Res:  79.70% (  72969/  91551)
	    3  Both:  62.07% (  46378/  74714)  Adr:  76.36% (  57051/  74714)  Res:  79.79% (  59618/  74714)
	    4  Both:  60.52% (  47784/  78954)  Adr:  74.19% (  58579/  78954)  Res:  80.16% (  63287/  78954)
	    5  Both:  60.45% (  60141/  99481)  Adr:  70.41% (  70044/  99481)  Res:  84.20% (  83767/  99481)
	    6  Both:  56.92% ( 115063/ 202159)  Adr:  65.31% ( 132034/ 202159)  Res:  85.05% ( 171934/ 202159)


  DEV    100/1264  200/1264  300/1264  400/1264  500/1264  600/1264  700/1264  800/1264  900/1264  1000/1264  1100/1264  1200/1264
	Time: 27.111409
	Total Loss: 0.000000	Total Grad Norm: 0.000000
	Avg.  Loss: 0.000000	Avg.  Grad Norm: 0.000000

	Accuracy
	TOTAL  Both:  55.63% (  21509/  38661)  Adr:  69.85% (  27006/  38661)  Res:  77.95% (  30135/  38661)

	    0  Both:  65.49% (   2025/   3092)  Adr:  87.68% (   2711/   3092)  Res:  74.48% (   2303/   3092)
	    1  Both:  60.23% (   3227/   5358)  Adr:  78.52% (   4207/   5358)  Res:  75.42% (   4041/   5358)
	    2  Both:  54.66% (   3193/   5842)  Adr:  71.29% (   4165/   5842)  Res:  75.57% (   4415/   5842)
	    3  Both:  53.71% (   2174/   4048)  Adr:  68.82% (   2786/   4048)  Res:  76.04% (   3078/   4048)
	    4  Both:  54.93% (   2574/   4686)  Adr:  69.65% (   3264/   4686)  Res:  75.97% (   3560/   4686)
	    5  Both:  54.11% (   3230/   5969)  Adr:  64.90% (   3874/   5969)  Res:  80.31% (   4794/   5969)
	    6  Both:  52.62% (   5086/   9666)  Adr:  62.06% (   5999/   9666)  Res:  82.18% (   7944/   9666)


  TEST    100/1453  200/1453  300/1453  400/1453  500/1453  600/1453  700/1453  800/1453  900/1453  1000/1453  1100/1453  1200/1453  1300/1453  1400/1453
	Time: 30.918288
	Total Loss: 0.000000	Total Grad Norm: 0.000000
	Avg.  Loss: 0.000000	Avg.  Grad Norm: 0.000000

	Accuracy
	TOTAL  Both:  54.72% (  24468/  44714)  Adr:  67.98% (  30395/  44714)  Res:  78.85% (  35257/  44714)

	    0  Both:  66.77% (   2080/   3115)  Adr:  88.54% (   2758/   3115)  Res:  74.93% (   2334/   3115)
	    1  Both:  58.03% (   3126/   5387)  Adr:  75.74% (   4080/   5387)  Res:  76.03% (   4096/   5387)
	    2  Both:  55.75% (   2864/   5137)  Adr:  72.38% (   3718/   5137)  Res:  74.91% (   3848/   5137)
	    3  Both:  54.28% (   2367/   4361)  Adr:  70.30% (   3066/   4361)  Res:  75.79% (   3305/   4361)
	    4  Both:  52.57% (   2342/   4455)  Adr:  66.55% (   2965/   4455)  Res:  76.97% (   3429/   4455)
	    5  Both:  56.70% (   3356/   5919)  Adr:  67.51% (   3996/   5919)  Res:  81.82% (   4843/   5919)
	    6  Both:  51.00% (   8333/  16340)  Adr:  60.05% (   9812/  16340)  Res:  82.02% (  13402/  16340)

	BEST ACCURACY HISTORY
	EPOCH-  1 | DEV  Both: 52.46%  Adr: 68.70%  Res: 75.17% | TEST  Both: 51.17%  Adr: 66.27%  Res: 75.89%
	EPOCH-  2 | DEV  Both: 54.32%  Adr: 69.44%  Res: 76.87% | TEST  Both: 53.53%  Adr: 67.33%  Res: 78.08%
	EPOCH-  3 | DEV  Both: 55.39%  Adr: 69.64%  Res: 77.85% | TEST  Both: 54.37%  Adr: 67.91%  Res: 78.46%
	EPOCH-  4 | DEV  Both: 55.63%  Adr: 69.85%  Res: 77.95% | TEST  Both: 54.72%  Adr: 67.98%  Res: 78.85%


Epoch: 5
  TRAIN    100/21206  200/21206  300/21206  400/21206  500/21206  600/21206  700/21206  800/21206  900/21206  1000/21206  1100/21206  1200/21206  1300/21206  1400/21206  1500/21206  1600/21206  1700/21206  1800/21206  1900/21206  2000/21206  2100/21206  2200/21206  2300/21206  2400/21206  2500/21206  2600/21206  2700/21206  2800/21206  2900/21206  3000/21206  3100/21206  3200/21206  3300/21206  3400/21206  3500/21206  3600/21206  3700/21206  3800/21206  3900/21206  4000/21206  4100/21206  4200/21206  4300/21206  4400/21206  4500/21206  4600/21206  4700/21206  4800/21206  4900/21206  5000/21206  5100/21206  5200/21206  5300/21206  5400/21206  5500/21206  5600/21206  5700/21206  5800/21206  5900/21206  6000/21206  6100/21206  6200/21206  6300/21206  6400/21206  6500/21206  6600/21206  6700/21206  6800/21206  6900/21206  7000/21206  7100/21206  7200/21206  7300/21206  7400/21206  7500/21206  7600/21206  7700/21206  7800/21206  7900/21206  8000/21206  8100/21206  8200/21206  8300/21206  8400/21206  8500/21206  8600/21206  8700/21206  8800/21206  8900/21206  9000/21206  9100/21206  9200/21206  9300/21206  9400/21206  9500/21206  9600/21206  9700/21206  9800/21206  9900/21206  10000/21206  10100/21206  10200/21206  10300/21206  10400/21206  10500/21206  10600/21206  10700/21206  10800/21206  10900/21206  11000/21206  11100/21206  11200/21206  11300/21206  11400/21206  11500/21206  11600/21206  11700/21206  11800/21206  11900/21206  12000/21206  12100/21206  12200/21206  12300/21206  12400/21206  12500/21206  12600/21206  12700/21206  12800/21206  12900/21206  13000/21206  13100/21206  13200/21206  13300/21206  13400/21206  13500/21206  13600/21206  13700/21206  13800/21206  13900/21206  14000/21206  14100/21206  14200/21206  14300/21206  14400/21206  14500/21206  14600/21206  14700/21206  14800/21206  14900/21206  15000/21206  15100/21206  15200/21206  15300/21206  15400/21206  15500/21206  15600/21206  15700/21206  15800/21206  15900/21206  16000/21206  16100/21206  16200/21206  16300/21206  16400/21206  16500/21206  16600/21206  16700/21206  16800/21206  16900/21206  17000/21206  17100/21206  17200/21206  17300/21206  17400/21206  17500/21206  17600/21206  17700/21206  17800/21206  17900/21206  18000/21206  18100/21206  18200/21206  18300/21206  18400/21206  18500/21206  18600/21206  18700/21206  18800/21206  18900/21206  19000/21206  19100/21206  19200/21206  19300/21206  19400/21206  19500/21206  19600/21206  19700/21206  19800/21206  19900/21206  20000/21206  20100/21206  20200/21206  20300/21206  20400/21206  20500/21206  20600/21206  20700/21206  20800/21206  20900/21206  21000/21206  21100/21206  21200/21206
	Time: 1546.372182
	Total Loss: 648869.074573	Total Grad Norm: 217061.981818
	Avg.  Loss: 30.598372	Avg.  Grad Norm: 10.235876

	Accuracy
	TOTAL  Both:  62.90% ( 426834/ 678592)  Adr:  74.05% ( 502474/ 678592)  Res:  83.73% ( 568217/ 678592)

	    0  Both:  71.87% (  27482/  38241)  Adr:  88.09% (  33686/  38241)  Res:  81.08% (  31007/  38241)
	    1  Both:  66.09% (  61790/  93492)  Adr:  80.37% (  75141/  93492)  Res:  81.22% (  75936/  93492)
	    2  Both:  64.18% (  58760/  91551)  Adr:  77.46% (  70915/  91551)  Res:  81.79% (  74879/  91551)
	    3  Both:  64.16% (  47937/  74714)  Adr:  77.12% (  57616/  74714)  Res:  81.83% (  61142/  74714)
	    4  Both:  62.70% (  49503/  78954)  Adr:  75.17% (  59353/  78954)  Res:  82.14% (  64853/  78954)
	    5  Both:  62.55% (  62227/  99481)  Adr:  71.55% (  71180/  99481)  Res:  85.76% (  85316/  99481)
	    6  Both:  58.93% ( 119135/ 202159)  Adr:  66.57% ( 134583/ 202159)  Res:  86.61% ( 175084/ 202159)


  DEV    100/1264  200/1264  300/1264  400/1264  500/1264  600/1264  700/1264  800/1264  900/1264  1000/1264  1100/1264  1200/1264
	Time: 26.471592
	Total Loss: 0.000000	Total Grad Norm: 0.000000
	Avg.  Loss: 0.000000	Avg.  Grad Norm: 0.000000

	Accuracy
	TOTAL  Both:  55.14% (  21317/  38661)  Adr:  69.70% (  26946/  38661)  Res:  77.60% (  30000/  38661)

	    0  Both:  65.10% (   2013/   3092)  Adr:  87.45% (   2704/   3092)  Res:  74.22% (   2295/   3092)
	    1  Both:  59.50% (   3188/   5358)  Adr:  78.18% (   4189/   5358)  Res:  75.12% (   4025/   5358)
	    2  Both:  54.19% (   3166/   5842)  Adr:  71.26% (   4163/   5842)  Res:  74.91% (   4376/   5842)
	    3  Both:  52.92% (   2142/   4048)  Adr:  68.45% (   2771/   4048)  Res:  75.67% (   3063/   4048)
	    4  Both:  54.61% (   2559/   4686)  Adr:  69.65% (   3264/   4686)  Res:  75.67% (   3546/   4686)
	    5  Both:  53.59% (   3199/   5969)  Adr:  64.83% (   3870/   5969)  Res:  80.21% (   4788/   5969)
	    6  Both:  52.24% (   5050/   9666)  Adr:  61.92% (   5985/   9666)  Res:  81.80% (   7907/   9666)


  TEST    100/1453  200/1453  300/1453  400/1453  500/1453  600/1453  700/1453  800/1453  900/1453  1000/1453  1100/1453  1200/1453  1300/1453  1400/1453
	Time: 30.529806
	Total Loss: 0.000000	Total Grad Norm: 0.000000
	Avg.  Loss: 0.000000	Avg.  Grad Norm: 0.000000

	Accuracy
	TOTAL  Both:  54.31% (  24282/  44714)  Adr:  67.73% (  30284/  44714)  Res:  78.44% (  35073/  44714)

	    0  Both:  66.87% (   2083/   3115)  Adr:  87.67% (   2731/   3115)  Res:  75.44% (   2350/   3115)
	    1  Both:  57.88% (   3118/   5387)  Adr:  75.83% (   4085/   5387)  Res:  75.24% (   4053/   5387)
	    2  Both:  54.78% (   2814/   5137)  Adr:  71.81% (   3689/   5137)  Res:  74.42% (   3823/   5137)
	    3  Both:  53.86% (   2349/   4361)  Adr:  69.98% (   3052/   4361)  Res:  75.65% (   3299/   4361)
	    4  Both:  51.56% (   2297/   4455)  Adr:  66.13% (   2946/   4455)  Res:  76.05% (   3388/   4455)
	    5  Both:  57.17% (   3384/   5919)  Adr:  67.71% (   4008/   5919)  Res:  81.84% (   4844/   5919)
	    6  Both:  50.41% (   8237/  16340)  Adr:  59.81% (   9773/  16340)  Res:  81.49% (  13316/  16340)

	BEST ACCURACY HISTORY
	EPOCH-  1 | DEV  Both: 52.46%  Adr: 68.70%  Res: 75.17% | TEST  Both: 51.17%  Adr: 66.27%  Res: 75.89%
	EPOCH-  2 | DEV  Both: 54.32%  Adr: 69.44%  Res: 76.87% | TEST  Both: 53.53%  Adr: 67.33%  Res: 78.08%
	EPOCH-  3 | DEV  Both: 55.39%  Adr: 69.64%  Res: 77.85% | TEST  Both: 54.37%  Adr: 67.91%  Res: 78.46%
	EPOCH-  4 | DEV  Both: 55.63%  Adr: 69.85%  Res: 77.95% | TEST  Both: 54.72%  Adr: 67.98%  Res: 78.85%


Epoch: 6
  TRAIN    100/21206  200/21206  300/21206  400/21206  500/21206  600/21206  700/21206  800/21206  900/21206  1000/21206  1100/21206  1200/21206  1300/21206  1400/21206  1500/21206  1600/21206  1700/21206  1800/21206  1900/21206  2000/21206  2100/21206  2200/21206  2300/21206  2400/21206  2500/21206  2600/21206  2700/21206  2800/21206  2900/21206  3000/21206  3100/21206  3200/21206  3300/21206  3400/21206  3500/21206  3600/21206  3700/21206  3800/21206  3900/21206  4000/21206  4100/21206  4200/21206  4300/21206  4400/21206  4500/21206  4600/21206  4700/21206  4800/21206  4900/21206  5000/21206  5100/21206  5200/21206  5300/21206  5400/21206  5500/21206  5600/21206  5700/21206  5800/21206  5900/21206  6000/21206  6100/21206  6200/21206  6300/21206  6400/21206  6500/21206  6600/21206  6700/21206  6800/21206  6900/21206

Loss is NAN: Mini-Batch Index: 6933
