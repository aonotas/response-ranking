WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:
 https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29

Using gpu device 0: GeForce GTX 1070 (CNMeM is disabled, cuDNN 5105)

Namespace(activation='tanh', attention=0, batch=32, data_size=10000000, dev_data='/cl/work/motoki-s/multi_ling_conversation/data/2015_concat/2015_cand2_lang_en_dev.txt', dim_emb=50, dim_hidden=50, emb_type='mono', epoch=30, init_emb=None, lang='en', load_param=None, loss='nll', lr=0.001, max_n_words=20, mode='train', model='dynamic', n_cands=2, n_prev_sents=5, opt='adam', output=0, output_fn='dynamic-gru_lang_en_cand2', reg=0.0001, sample_size=1, save=1, test_data='/cl/work/motoki-s/multi_ling_conversation/data/2015_concat/2015_cand2_lang_en_test.txt', train_data='/cl/work/motoki-s/multi_ling_conversation/data/2015_concat/2015_cand2_lang_en_train.txt', unit='gru')


ADDRESSEE AND RESPONSE SELECTION SYSTEM START

SET UP DATASET

Load dataset...
Load initial word embedding...
	Random Initialized Word Embeddings

TASK  SETTING
	Response Candidates:2  Contexts:5  Max Word Num:20


Converting words into ids...
	Questions:   751108
	Questions:    39809
	Questions:    45776

Creating samples...
	THREADS:  6606
	  SAMPLES:   636547
	  ADDRESSEE DETECTION CHANCE LEVEL:   1.50%
	  ADDRESSEE DETECTION UPPER BOUND:
		# Cands  1:   0.00% | Total:        0 | Including true-adr:        0 | Not including:        0
		# Cands  2: 100.00% | Total:   157527 | Including true-adr:   157527 | Not including:        0
		# Cands  3: 100.00% | Total:   220381 | Including true-adr:   220381 | Not including:        0
		# Cands  4: 100.00% | Total:   172959 | Including true-adr:   172959 | Not including:        0
		# Cands  5: 100.00% | Total:    72895 | Including true-adr:    72895 | Not including:        0
		# Cands  6: 100.00% | Total:    12785 | Including true-adr:    12785 | Not including:        0

	  THE BINNED NUMBER OF AGENTS IN CONTEXT:
		Bin  0:    52584
		Bin  1:    90113
		Bin  2:    84726
		Bin  3:    68841
		Bin  4:    72401
		Bin  5:    90504
		Bin  6:   177378

	THREADS:   367
	  SAMPLES:    38661
	  ADDRESSEE DETECTION CHANCE LEVEL:   1.59%
	  ADDRESSEE DETECTION UPPER BOUND:
		# Cands  1:   0.00% | Total:      119 | Including true-adr:        0 | Not including:      119
		# Cands  2:  92.27% | Total:     9698 | Including true-adr:     8948 | Not including:      750
		# Cands  3:  87.68% | Total:    13337 | Including true-adr:    11694 | Not including:     1643
		# Cands  4:  86.75% | Total:    10341 | Including true-adr:     8971 | Not including:     1370
		# Cands  5:  85.26% | Total:     4382 | Including true-adr:     3736 | Not including:      646
		# Cands  6:  85.84% | Total:      784 | Including true-adr:      673 | Not including:      111

	  THE BINNED NUMBER OF AGENTS IN CONTEXT:
		Bin  0:     3092
		Bin  1:     5358
		Bin  2:     5842
		Bin  3:     4048
		Bin  4:     4686
		Bin  5:     5969
		Bin  6:     9666

	THREADS:   382
	  SAMPLES:    44714
	  ADDRESSEE DETECTION CHANCE LEVEL:   1.23%
	  ADDRESSEE DETECTION UPPER BOUND:
		# Cands  1:   0.00% | Total:      147 | Including true-adr:        0 | Not including:      147
		# Cands  2:  92.04% | Total:     9593 | Including true-adr:     8829 | Not including:      764
		# Cands  3:  87.66% | Total:    14982 | Including true-adr:    13133 | Not including:     1849
		# Cands  4:  86.13% | Total:    12946 | Including true-adr:    11150 | Not including:     1796
		# Cands  5:  83.88% | Total:     5899 | Including true-adr:     4948 | Not including:      951
		# Cands  6:  84.74% | Total:     1147 | Including true-adr:      972 | Not including:      175

	  THE BINNED NUMBER OF AGENTS IN CONTEXT:
		Bin  0:     3115
		Bin  1:     5387
		Bin  2:     5137
		Bin  3:     4361
		Bin  4:     4455
		Bin  5:     5919
		Bin  6:    16340


TRAIN SETTING	Batch Size:32  Epoch:30  Vocab:498122  Max Words:20

Train samples	Mini-Batch:19888
Dev samples	Mini-Batch:1216
Test samples	Mini-Batch:1405

BUILD A MODEL
MODEL: dynamic  Unit: gru  Opt: adam  Activation: tanh  Parameters: 24946050


TRAINING START



Epoch: 1
  TRAIN    100/19888  200/19888  300/19888  400/19888  500/19888  600/19888  700/19888  800/19888  900/19888  1000/19888  1100/19888  1200/19888  1300/19888  1400/19888  1500/19888  1600/19888  1700/19888  1800/19888  1900/19888  2000/19888  2100/19888  2200/19888  2300/19888  2400/19888  2500/19888  2600/19888  2700/19888  2800/19888  2900/19888  3000/19888  3100/19888  3200/19888  3300/19888  3400/19888  3500/19888  3600/19888  3700/19888  3800/19888  3900/19888  4000/19888  4100/19888  4200/19888  4300/19888  4400/19888  4500/19888  4600/19888  4700/19888  4800/19888  4900/19888  5000/19888  5100/19888  5200/19888  5300/19888  5400/19888  5500/19888  5600/19888  5700/19888  5800/19888  5900/19888  6000/19888  6100/19888  6200/19888  6300/19888  6400/19888  6500/19888  6600/19888  6700/19888  6800/19888  6900/19888  7000/19888  7100/19888  7200/19888  7300/19888  7400/19888  7500/19888  7600/19888  7700/19888  7800/19888  7900/19888  8000/19888  8100/19888  8200/19888  8300/19888  8400/19888  8500/19888  8600/19888  8700/19888  8800/19888  8900/19888  9000/19888  9100/19888  9200/19888  9300/19888  9400/19888  9500/19888  9600/19888  9700/19888  9800/19888  9900/19888  10000/19888  10100/19888  10200/19888  10300/19888  10400/19888  10500/19888  10600/19888  10700/19888  10800/19888  10900/19888  11000/19888  11100/19888  11200/19888  11300/19888  11400/19888  11500/19888  11600/19888  11700/19888  11800/19888  11900/19888  12000/19888  12100/19888  12200/19888  12300/19888  12400/19888  12500/19888  12600/19888  12700/19888  12800/19888  12900/19888  13000/19888  13100/19888  13200/19888  13300/19888  13400/19888  13500/19888  13600/19888  13700/19888  13800/19888  13900/19888  14000/19888  14100/19888  14200/19888  14300/19888  14400/19888  14500/19888  14600/19888  14700/19888  14800/19888  14900/19888  15000/19888  15100/19888  15200/19888  15300/19888  15400/19888  15500/19888  15600/19888  15700/19888  15800/19888  15900/19888  16000/19888  16100/19888  16200/19888  16300/19888  16400/19888  16500/19888  16600/19888  16700/19888  16800/19888  16900/19888  17000/19888  17100/19888  17200/19888  17300/19888  17400/19888  17500/19888  17600/19888  17700/19888  17800/19888  17900/19888  18000/19888  18100/19888  18200/19888  18300/19888  18400/19888  18500/19888  18600/19888  18700/19888  18800/19888  18900/19888  19000/19888  19100/19888  19200/19888  19300/19888  19400/19888  19500/19888  19600/19888  19700/19888  19800/19888
	Time: 1200.278653
	Total Loss: 693776.158463	Total Grad Norm: 163963.303550
	Avg.  Loss: 34.884159	Avg.  Grad Norm: 8.244333

	Accuracy
	TOTAL  Both:  48.82% ( 310694/ 636416)  Adr:  71.18% ( 453008/ 636416)  Res:  67.87% ( 431959/ 636416)

	    0  Both:  59.00% (  31024/  52580)  Adr:  88.34% (  46448/  52580)  Res:  66.50% (  34965/  52580)
	    1  Both:  54.14% (  48775/  90098)  Adr:  80.60% (  72617/  90098)  Res:  66.55% (  59964/  90098)
	    2  Both:  51.77% (  43855/  84714)  Adr:  77.48% (  65635/  84714)  Res:  66.34% (  56198/  84714)
	    3  Both:  51.40% (  35383/  68833)  Adr:  76.74% (  52823/  68833)  Res:  66.17% (  45549/  68833)
	    4  Both:  49.87% (  36104/  72394)  Adr:  74.12% (  53657/  72394)  Res:  66.56% (  48185/  72394)
	    5  Both:  46.37% (  41957/  90479)  Adr:  65.75% (  59492/  90479)  Res:  69.16% (  62576/  90479)
	    6  Both:  41.51% (  73596/ 177318)  Adr:  57.71% ( 102336/ 177318)  Res:  70.23% ( 124522/ 177318)


  DEV    100/1216  200/1216  300/1216  400/1216  500/1216  600/1216  700/1216  800/1216  900/1216  1000/1216  1100/1216  1200/1216
	Time: 7.342823
	Total Loss: 0.000000	Total Grad Norm: 0.000000
	Avg.  Loss: 0.000000	Avg.  Grad Norm: 0.000000

	Accuracy
	TOTAL  Both:  46.89% (  18127/  38661)  Adr:  64.56% (  24958/  38661)  Res:  70.84% (  27389/  38661)

	    0  Both:  60.87% (   1882/   3092)  Adr:  86.29% (   2668/   3092)  Res:  70.21% (   2171/   3092)
	    1  Both:  54.50% (   2920/   5358)  Adr:  76.35% (   4091/   5358)  Res:  69.84% (   3742/   5358)
	    2  Both:  49.50% (   2892/   5842)  Adr:  69.70% (   4072/   5842)  Res:  69.07% (   4035/   5842)
	    3  Both:  47.31% (   1915/   4048)  Adr:  66.06% (   2674/   4048)  Res:  69.99% (   2833/   4048)
	    4  Both:  48.48% (   2272/   4686)  Adr:  67.07% (   3143/   4686)  Res:  69.72% (   3267/   4686)
	    5  Both:  42.29% (   2524/   5969)  Adr:  57.28% (   3419/   5969)  Res:  71.67% (   4278/   5969)
	    6  Both:  38.51% (   3722/   9666)  Adr:  50.60% (   4891/   9666)  Res:  73.07% (   7063/   9666)


  TEST    100/1405  200/1405  300/1405  400/1405  500/1405  600/1405  700/1405  800/1405  900/1405  1000/1405  1100/1405  1200/1405  1300/1405  1400/1405
	Time: 7.897457
	Total Loss: 0.000000	Total Grad Norm: 0.000000
	Avg.  Loss: 0.000000	Avg.  Grad Norm: 0.000000

	Accuracy
	TOTAL  Both:  45.02% (  20131/  44714)  Adr:  61.55% (  27523/  44714)  Res:  71.42% (  31935/  44714)

	    0  Both:  61.57% (   1918/   3115)  Adr:  86.00% (   2679/   3115)  Res:  71.01% (   2212/   3115)
	    1  Both:  51.96% (   2799/   5387)  Adr:  73.55% (   3962/   5387)  Res:  70.11% (   3777/   5387)
	    2  Both:  50.28% (   2583/   5137)  Adr:  70.04% (   3598/   5137)  Res:  69.77% (   3584/   5137)
	    3  Both:  47.67% (   2079/   4361)  Adr:  67.58% (   2947/   4361)  Res:  69.18% (   3017/   4361)
	    4  Both:  45.86% (   2043/   4455)  Adr:  64.11% (   2856/   4455)  Res:  69.38% (   3091/   4455)
	    5  Both:  45.41% (   2688/   5919)  Adr:  59.66% (   3531/   5919)  Res:  73.12% (   4328/   5919)
	    6  Both:  36.85% (   6021/  16340)  Adr:  48.65% (   7950/  16340)  Res:  72.99% (  11926/  16340)

	BEST ACCURACY HISTORY
	EPOCH-  1 | DEV  Both: 46.89%  Adr: 64.56%  Res: 70.84% | TEST  Both: 45.02%  Adr: 61.55%  Res: 71.42%


Epoch: 2
  TRAIN    100/19888  200/19888  300/19888  400/19888  500/19888  600/19888  700/19888  800/19888  900/19888  1000/19888  1100/19888  1200/19888  1300/19888  1400/19888  1500/19888  1600/19888  1700/19888  1800/19888  1900/19888  2000/19888  2100/19888  2200/19888  2300/19888  2400/19888  2500/19888  2600/19888  2700/19888  2800/19888  2900/19888  3000/19888  3100/19888  3200/19888  3300/19888  3400/19888  3500/19888  3600/19888  3700/19888  3800/19888  3900/19888  4000/19888  4100/19888  4200/19888  4300/19888  4400/19888  4500/19888  4600/19888  4700/19888  4800/19888  4900/19888  5000/19888  5100/19888  5200/19888  5300/19888  5400/19888  5500/19888  5600/19888  5700/19888  5800/19888  5900/19888  6000/19888  6100/19888  6200/19888  6300/19888  6400/19888  6500/19888  6600/19888  6700/19888  6800/19888  6900/19888  7000/19888  7100/19888  7200/19888  7300/19888  7400/19888  7500/19888  7600/19888  7700/19888  7800/19888  7900/19888  8000/19888  8100/19888  8200/19888  8300/19888  8400/19888  8500/19888  8600/19888  8700/19888  8800/19888  8900/19888  9000/19888  9100/19888  9200/19888  9300/19888  9400/19888  9500/19888  9600/19888  9700/19888  9800/19888  9900/19888  10000/19888  10100/19888  10200/19888  10300/19888  10400/19888  10500/19888  10600/19888  10700/19888  10800/19888  10900/19888  11000/19888  11100/19888  11200/19888  11300/19888  11400/19888  11500/19888  11600/19888  11700/19888  11800/19888  11900/19888  12000/19888  12100/19888  12200/19888  12300/19888  12400/19888  12500/19888  12600/19888  12700/19888  12800/19888  12900/19888  13000/19888  13100/19888  13200/19888  13300/19888  13400/19888  13500/19888  13600/19888  13700/19888  13800/19888  13900/19888  14000/19888  14100/19888  14200/19888  14300/19888  14400/19888  14500/19888  14600/19888  14700/19888  14800/19888  14900/19888  15000/19888  15100/19888  15200/19888  15300/19888  15400/19888  15500/19888  15600/19888  15700/19888  15800/19888  15900/19888  16000/19888  16100/19888  16200/19888  16300/19888  16400/19888  16500/19888  16600/19888  16700/19888  16800/19888  16900/19888  17000/19888  17100/19888  17200/19888  17300/19888  17400/19888  17500/19888  17600/19888  17700/19888  17800/19888  17900/19888  18000/19888  18100/19888  18200/19888  18300/19888  18400/19888  18500/19888  18600/19888  18700/19888  18800/19888  18900/19888  19000/19888  19100/19888  19200/19888  19300/19888  19400/19888  19500/19888  19600/19888  19700/19888  19800/19888
	Time: 1186.918400
	Total Loss: 656545.760798	Total Grad Norm: 211277.308649
	Avg.  Loss: 33.012156	Avg.  Grad Norm: 10.623356

	Accuracy
	TOTAL  Both:  54.70% ( 348145/ 636416)  Adr:  73.13% ( 465422/ 636416)  Res:  73.66% ( 468766/ 636416)

	    0  Both:  64.37% (  33845/  52580)  Adr:  88.89% (  46739/  52580)  Res:  71.99% (  37852/  52580)
	    1  Both:  59.59% (  53692/  90098)  Adr:  81.75% (  73657/  90098)  Res:  72.06% (  64929/  90098)
	    2  Both:  57.52% (  48728/  84714)  Adr:  78.84% (  66791/  84714)  Res:  72.05% (  61038/  84714)
	    3  Both:  57.34% (  39470/  68833)  Adr:  78.26% (  53868/  68833)  Res:  72.13% (  49648/  68833)
	    4  Both:  55.47% (  40158/  72394)  Adr:  75.50% (  54659/  72394)  Res:  72.30% (  52340/  72394)
	    5  Both:  52.95% (  47908/  90479)  Adr:  68.40% (  61885/  90479)  Res:  75.41% (  68233/  90479)
	    6  Both:  47.57% (  84344/ 177318)  Adr:  60.81% ( 107823/ 177318)  Res:  75.98% ( 134726/ 177318)


  DEV    100/1216  200/1216  300/1216  400/1216  500/1216  600/1216  700/1216  800/1216  900/1216  1000/1216  1100/1216  1200/1216
	Time: 7.159421
	Total Loss: 0.000000	Total Grad Norm: 0.000000
	Avg.  Loss: 0.000000	Avg.  Grad Norm: 0.000000

	Accuracy
	TOTAL  Both:  48.67% (  18817/  38661)  Adr:  65.12% (  25176/  38661)  Res:  72.93% (  28195/  38661)

	    0  Both:  61.77% (   1910/   3092)  Adr:  86.19% (   2665/   3092)  Res:  71.22% (   2202/   3092)
	    1  Both:  56.53% (   3029/   5358)  Adr:  76.56% (   4102/   5358)  Res:  73.01% (   3912/   5358)
	    2  Both:  50.19% (   2932/   5842)  Adr:  69.75% (   4075/   5842)  Res:  70.73% (   4132/   5842)
	    3  Both:  48.62% (   1968/   4048)  Adr:  66.25% (   2682/   4048)  Res:  71.62% (   2899/   4048)
	    4  Both:  49.64% (   2326/   4686)  Adr:  66.73% (   3127/   4686)  Res:  71.94% (   3371/   4686)
	    5  Both:  45.90% (   2740/   5969)  Adr:  59.04% (   3524/   5969)  Res:  74.47% (   4445/   5969)
	    6  Both:  40.47% (   3912/   9666)  Adr:  51.74% (   5001/   9666)  Res:  74.84% (   7234/   9666)


  TEST    100/1405  200/1405  300/1405  400/1405  500/1405  600/1405  700/1405  800/1405  900/1405  1000/1405  1100/1405  1200/1405  1300/1405  1400/1405
	Time: 7.947588
	Total Loss: 0.000000	Total Grad Norm: 0.000000
	Avg.  Loss: 0.000000	Avg.  Grad Norm: 0.000000

	Accuracy
	TOTAL  Both:  46.79% (  20922/  44714)  Adr:  62.26% (  27841/  44714)  Res:  73.09% (  32682/  44714)

	    0  Both:  62.25% (   1939/   3115)  Adr:  85.84% (   2674/   3115)  Res:  72.52% (   2259/   3115)
	    1  Both:  53.50% (   2882/   5387)  Adr:  73.83% (   3977/   5387)  Res:  71.99% (   3878/   5387)
	    2  Both:  52.21% (   2682/   5137)  Adr:  70.24% (   3608/   5137)  Res:  71.54% (   3675/   5137)
	    3  Both:  48.48% (   2114/   4361)  Adr:  67.30% (   2935/   4361)  Res:  70.65% (   3081/   4361)
	    4  Both:  47.30% (   2107/   4455)  Adr:  63.82% (   2843/   4455)  Res:  71.38% (   3180/   4455)
	    5  Both:  47.76% (   2827/   5919)  Adr:  60.94% (   3607/   5919)  Res:  74.88% (   4432/   5919)
	    6  Both:  38.99% (   6371/  16340)  Adr:  50.17% (   8197/  16340)  Res:  74.52% (  12177/  16340)

	BEST ACCURACY HISTORY
	EPOCH-  1 | DEV  Both: 46.89%  Adr: 64.56%  Res: 70.84% | TEST  Both: 45.02%  Adr: 61.55%  Res: 71.42%
	EPOCH-  2 | DEV  Both: 48.67%  Adr: 65.12%  Res: 72.93% | TEST  Both: 46.79%  Adr: 62.26%  Res: 73.09%


Epoch: 3
  TRAIN    100/19888  200/19888  300/19888  400/19888  500/19888  600/19888  700/19888  800/19888  900/19888  1000/19888  1100/19888  1200/19888  1300/19888  1400/19888  1500/19888  1600/19888  1700/19888  1800/19888  1900/19888  2000/19888  2100/19888  2200/19888  2300/19888  2400/19888  2500/19888  2600/19888  2700/19888  2800/19888  2900/19888  3000/19888  3100/19888  3200/19888  3300/19888  3400/19888  3500/19888  3600/19888  3700/19888  3800/19888  3900/19888  4000/19888  4100/19888  4200/19888  4300/19888  4400/19888  4500/19888  4600/19888  4700/19888  4800/19888  4900/19888  5000/19888  5100/19888  5200/19888  5300/19888  5400/19888  5500/19888  5600/19888  5700/19888  5800/19888  5900/19888  6000/19888  6100/19888  6200/19888  6300/19888  6400/19888  6500/19888  6600/19888  6700/19888  6800/19888  6900/19888  7000/19888  7100/19888  7200/19888  7300/19888  7400/19888  7500/19888  7600/19888  7700/19888  7800/19888  7900/19888  8000/19888  8100/19888  8200/19888  8300/19888  8400/19888  8500/19888  8600/19888  8700/19888  8800/19888  8900/19888  9000/19888  9100/19888  9200/19888  9300/19888  9400/19888  9500/19888  9600/19888  9700/19888  9800/19888  9900/19888  10000/19888  10100/19888  10200/19888  10300/19888  10400/19888  10500/19888  10600/19888  10700/19888  10800/19888  10900/19888  11000/19888  11100/19888  11200/19888  11300/19888  11400/19888  11500/19888  11600/19888  11700/19888  11800/19888  11900/19888  12000/19888  12100/19888  12200/19888  12300/19888  12400/19888  12500/19888  12600/19888  12700/19888  12800/19888  12900/19888  13000/19888  13100/19888  13200/19888  13300/19888  13400/19888  13500/19888  13600/19888  13700/19888  13800/19888  13900/19888  14000/19888  14100/19888  14200/19888  14300/19888  14400/19888  14500/19888  14600/19888  14700/19888  14800/19888  14900/19888  15000/19888  15100/19888  15200/19888  15300/19888  15400/19888  15500/19888  15600/19888  15700/19888  15800/19888  15900/19888  16000/19888  16100/19888  16200/19888  16300/19888  16400/19888  16500/19888  16600/19888  16700/19888  16800/19888  16900/19888  17000/19888  17100/19888  17200/19888  17300/19888  17400/19888  17500/19888  17600/19888  17700/19888  17800/19888  17900/19888  18000/19888  18100/19888  18200/19888  18300/19888  18400/19888  18500/19888  18600/19888  18700/19888  18800/19888  18900/19888  19000/19888  19100/19888  19200/19888  19300/19888  19400/19888  19500/19888  19600/19888  19700/19888  19800/19888
	Time: 1180.144544
	Total Loss: 638363.676794	Total Grad Norm: 237652.419242
	Avg.  Loss: 32.097932	Avg.  Grad Norm: 11.949538

	Accuracy
	TOTAL  Both:  57.30% ( 364680/ 636416)  Adr:  74.03% ( 471166/ 636416)  Res:  76.16% ( 484666/ 636416)

	    0  Both:  66.76% (  35100/  52580)  Adr:  89.22% (  46914/  52580)  Res:  74.36% (  39099/  52580)
	    1  Both:  62.10% (  55953/  90098)  Adr:  82.36% (  74204/  90098)  Res:  74.52% (  67137/  90098)
	    2  Both:  60.00% (  50825/  84714)  Adr:  79.38% (  67250/  84714)  Res:  74.50% (  63109/  84714)
	    3  Both:  59.88% (  41217/  68833)  Adr:  78.92% (  54321/  68833)  Res:  74.61% (  51355/  68833)
	    4  Both:  58.05% (  42027/  72394)  Adr:  76.48% (  55365/  72394)  Res:  74.74% (  54105/  72394)
	    5  Both:  55.60% (  50308/  90479)  Adr:  69.50% (  62880/  90479)  Res:  77.89% (  70474/  90479)
	    6  Both:  50.33% (  89250/ 177318)  Adr:  62.17% ( 110232/ 177318)  Res:  78.61% ( 139387/ 177318)


  DEV    100/1216  200/1216  300/1216  400/1216  500/1216  600/1216  700/1216  800/1216  900/1216  1000/1216  1100/1216  1200/1216
	Time: 7.397696
	Total Loss: 0.000000	Total Grad Norm: 0.000000
	Avg.  Loss: 0.000000	Avg.  Grad Norm: 0.000000

	Accuracy
	TOTAL  Both:  49.46% (  19120/  38661)  Adr:  65.29% (  25241/  38661)  Res:  73.68% (  28485/  38661)

	    0  Both:  63.03% (   1949/   3092)  Adr:  86.29% (   2668/   3092)  Res:  72.93% (   2255/   3092)
	    1  Both:  56.55% (   3030/   5358)  Adr:  76.37% (   4092/   5358)  Res:  72.66% (   3893/   5358)
	    2  Both:  51.99% (   3037/   5842)  Adr:  69.87% (   4082/   5842)  Res:  71.91% (   4201/   5842)
	    3  Both:  49.46% (   2002/   4048)  Adr:  66.75% (   2702/   4048)  Res:  72.18% (   2922/   4048)
	    4  Both:  49.70% (   2329/   4686)  Adr:  66.45% (   3114/   4686)  Res:  72.11% (   3379/   4686)
	    5  Both:  45.90% (   2740/   5969)  Adr:  58.77% (   3508/   5969)  Res:  74.82% (   4466/   5969)
	    6  Both:  41.72% (   4033/   9666)  Adr:  52.50% (   5075/   9666)  Res:  76.24% (   7369/   9666)


  TEST    100/1405  200/1405  300/1405  400/1405  500/1405  600/1405  700/1405  800/1405  900/1405  1000/1405  1100/1405  1200/1405  1300/1405  1400/1405
	Time: 7.930969
	Total Loss: 0.000000	Total Grad Norm: 0.000000
	Avg.  Loss: 0.000000	Avg.  Grad Norm: 0.000000

	Accuracy
	TOTAL  Both:  47.28% (  21141/  44714)  Adr:  62.24% (  27832/  44714)  Res:  73.77% (  32987/  44714)

	    0  Both:  62.57% (   1949/   3115)  Adr:  85.87% (   2675/   3115)  Res:  72.46% (   2257/   3115)
	    1  Both:  53.80% (   2898/   5387)  Adr:  73.70% (   3970/   5387)  Res:  71.86% (   3871/   5387)
	    2  Both:  52.21% (   2682/   5137)  Adr:  70.26% (   3609/   5137)  Res:  71.91% (   3694/   5137)
	    3  Both:  49.35% (   2152/   4361)  Adr:  67.87% (   2960/   4361)  Res:  70.88% (   3091/   4361)
	    4  Both:  47.97% (   2137/   4455)  Adr:  64.04% (   2853/   4455)  Res:  71.63% (   3191/   4455)
	    5  Both:  48.07% (   2845/   5919)  Adr:  60.64% (   3589/   5919)  Res:  75.99% (   4498/   5919)
	    6  Both:  39.65% (   6478/  16340)  Adr:  50.04% (   8176/  16340)  Res:  75.80% (  12385/  16340)

	BEST ACCURACY HISTORY
	EPOCH-  1 | DEV  Both: 46.89%  Adr: 64.56%  Res: 70.84% | TEST  Both: 45.02%  Adr: 61.55%  Res: 71.42%
	EPOCH-  2 | DEV  Both: 48.67%  Adr: 65.12%  Res: 72.93% | TEST  Both: 46.79%  Adr: 62.26%  Res: 73.09%
	EPOCH-  3 | DEV  Both: 49.46%  Adr: 65.29%  Res: 73.68% | TEST  Both: 47.28%  Adr: 62.24%  Res: 73.77%


Epoch: 4
  TRAIN    100/19888  200/19888  300/19888  400/19888  500/19888  600/19888  700/19888  800/19888  900/19888  1000/19888  1100/19888  1200/19888  1300/19888  1400/19888  1500/19888  1600/19888  1700/19888  1800/19888  1900/19888  2000/19888  2100/19888  2200/19888  2300/19888  2400/19888  2500/19888  2600/19888  2700/19888  2800/19888  2900/19888  3000/19888  3100/19888  3200/19888  3300/19888  3400/19888  3500/19888  3600/19888  3700/19888  3800/19888  3900/19888  4000/19888  4100/19888  4200/19888  4300/19888  4400/19888  4500/19888  4600/19888  4700/19888  4800/19888  4900/19888  5000/19888  5100/19888  5200/19888  5300/19888  5400/19888  5500/19888  5600/19888  5700/19888  5800/19888  5900/19888  6000/19888  6100/19888  6200/19888  6300/19888  6400/19888  6500/19888  6600/19888  6700/19888  6800/19888  6900/19888  7000/19888  7100/19888  7200/19888  7300/19888  7400/19888  7500/19888  7600/19888  7700/19888  7800/19888  7900/19888  8000/19888  8100/19888  8200/19888  8300/19888  8400/19888  8500/19888  8600/19888  8700/19888  8800/19888  8900/19888  9000/19888  9100/19888  9200/19888  9300/19888  9400/19888  9500/19888  9600/19888  9700/19888  9800/19888  9900/19888  10000/19888  10100/19888  10200/19888  10300/19888  10400/19888  10500/19888  10600/19888  10700/19888  10800/19888  10900/19888  11000/19888  11100/19888  11200/19888  11300/19888  11400/19888  11500/19888  11600/19888  11700/19888  11800/19888  11900/19888  12000/19888  12100/19888  12200/19888  12300/19888  12400/19888  12500/19888  12600/19888  12700/19888  12800/19888  12900/19888  13000/19888  13100/19888  13200/19888  13300/19888  13400/19888  13500/19888  13600/19888  13700/19888  13800/19888  13900/19888  14000/19888  14100/19888  14200/19888  14300/19888  14400/19888  14500/19888  14600/19888  14700/19888  14800/19888  14900/19888  15000/19888  15100/19888  15200/19888  15300/19888  15400/19888  15500/19888  15600/19888  15700/19888  15800/19888  15900/19888  16000/19888  16100/19888  16200/19888  16300/19888  16400/19888  16500/19888  16600/19888  16700/19888  16800/19888  16900/19888  17000/19888  17100/19888  17200/19888  17300/19888  17400/19888  17500/19888  17600/19888  17700/19888  17800/19888  17900/19888  18000/19888  18100/19888  18200/19888  18300/19888  18400/19888  18500/19888  18600/19888  18700/19888  18800/19888  18900/19888  19000/19888  19100/19888  19200/19888  19300/19888  19400/19888  19500/19888  19600/19888  19700/19888  19800/19888
	Time: 1176.357566
	Total Loss: 622282.960961	Total Grad Norm: 267098.842156
	Avg.  Loss: 31.289369	Avg.  Grad Norm: 13.430151

	Accuracy
	TOTAL  Both:  59.26% ( 377160/ 636416)  Adr:  74.80% ( 476043/ 636416)  Res:  77.94% ( 496028/ 636416)

	    0  Both:  68.59% (  36064/  52580)  Adr:  89.64% (  47134/  52580)  Res:  75.93% (  39925/  52580)
	    1  Both:  63.98% (  57649/  90098)  Adr:  82.90% (  74689/  90098)  Res:  76.39% (  68822/  90098)
	    2  Both:  62.02% (  52541/  84714)  Adr:  80.11% (  67866/  84714)  Res:  76.34% (  64670/  84714)
	    3  Both:  61.62% (  42415/  68833)  Adr:  79.49% (  54713/  68833)  Res:  76.33% (  52543/  68833)
	    4  Both:  60.22% (  43598/  72394)  Adr:  77.18% (  55871/  72394)  Res:  76.70% (  55525/  72394)
	    5  Both:  57.58% (  52102/  90479)  Adr:  70.41% (  63710/  90479)  Res:  79.59% (  72010/  90479)
	    6  Both:  52.33% (  92791/ 177318)  Adr:  63.20% ( 112060/ 177318)  Res:  80.38% ( 142533/ 177318)


  DEV    100/1216  200/1216  300/1216  400/1216  500/1216  600/1216  700/1216  800/1216  900/1216  1000/1216  1100/1216  1200/1216
	Time: 7.271275
	Total Loss: 0.000000	Total Grad Norm: 0.000000
	Avg.  Loss: 0.000000	Avg.  Grad Norm: 0.000000

	Accuracy
	TOTAL  Both:  49.46% (  19123/  38661)  Adr:  65.27% (  25233/  38661)  Res:  73.89% (  28565/  38661)

	    0  Both:  62.58% (   1935/   3092)  Adr:  86.19% (   2665/   3092)  Res:  72.35% (   2237/   3092)
	    1  Both:  56.83% (   3045/   5358)  Adr:  76.48% (   4098/   5358)  Res:  72.79% (   3900/   5358)
	    2  Both:  51.44% (   3005/   5842)  Adr:  69.74% (   4074/   5842)  Res:  71.74% (   4191/   5842)
	    3  Both:  48.94% (   1981/   4048)  Adr:  66.43% (   2689/   4048)  Res:  71.67% (   2901/   4048)
	    4  Both:  49.91% (   2339/   4686)  Adr:  66.52% (   3117/   4686)  Res:  72.32% (   3389/   4686)
	    5  Both:  46.27% (   2762/   5969)  Adr:  58.52% (   3493/   5969)  Res:  76.28% (   4553/   5969)
	    6  Both:  41.96% (   4056/   9666)  Adr:  52.73% (   5097/   9666)  Res:  76.49% (   7394/   9666)


  TEST    100/1405  200/1405  300/1405  400/1405  500/1405  600/1405  700/1405  800/1405  900/1405  1000/1405  1100/1405  1200/1405  1300/1405  1400/1405
	Time: 7.983916
	Total Loss: 0.000000	Total Grad Norm: 0.000000
	Avg.  Loss: 0.000000	Avg.  Grad Norm: 0.000000

	Accuracy
	TOTAL  Both:  47.44% (  21212/  44714)  Adr:  62.28% (  27850/  44714)  Res:  73.95% (  33066/  44714)

	    0  Both:  62.92% (   1960/   3115)  Adr:  86.13% (   2683/   3115)  Res:  72.78% (   2267/   3115)
	    1  Both:  53.68% (   2892/   5387)  Adr:  73.23% (   3945/   5387)  Res:  71.84% (   3870/   5387)
	    2  Both:  52.31% (   2687/   5137)  Adr:  70.00% (   3596/   5137)  Res:  72.26% (   3712/   5137)
	    3  Both:  49.14% (   2143/   4361)  Adr:  67.37% (   2938/   4361)  Res:  70.97% (   3095/   4361)
	    4  Both:  46.62% (   2077/   4455)  Adr:  63.37% (   2823/   4455)  Res:  71.31% (   3177/   4455)
	    5  Both:  48.30% (   2859/   5919)  Adr:  61.19% (   3622/   5919)  Res:  75.64% (   4477/   5919)
	    6  Both:  40.35% (   6594/  16340)  Adr:  50.45% (   8243/  16340)  Res:  76.30% (  12468/  16340)

	BEST ACCURACY HISTORY
	EPOCH-  1 | DEV  Both: 46.89%  Adr: 64.56%  Res: 70.84% | TEST  Both: 45.02%  Adr: 61.55%  Res: 71.42%
	EPOCH-  2 | DEV  Both: 48.67%  Adr: 65.12%  Res: 72.93% | TEST  Both: 46.79%  Adr: 62.26%  Res: 73.09%
	EPOCH-  3 | DEV  Both: 49.46%  Adr: 65.29%  Res: 73.68% | TEST  Both: 47.28%  Adr: 62.24%  Res: 73.77%
	EPOCH-  4 | DEV  Both: 49.46%  Adr: 65.27%  Res: 73.89% | TEST  Both: 47.44%  Adr: 62.28%  Res: 73.95%


Epoch: 5
  TRAIN    100/19888  200/19888  300/19888  400/19888  500/19888  600/19888  700/19888  800/19888  900/19888